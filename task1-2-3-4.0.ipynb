{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xXwk0POnLxye"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import os\n","import numpy as np\n","from torch.utils.data import DataLoader, TensorDataset\n","import torch.nn.functional as F\n","from torch.optim import AdamW,Adam\n","import matplotlib.pyplot as plt\n","from google.colab import files\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"5vTKKqt_R1D3"},"outputs":[],"source":["files.upload()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pZwe5_71t8_U"},"outputs":[],"source":["#definition of fourier convolution\n","\n","class SpectralConv1d(nn.Module):\n","    def __init__(self, in_channels, out_channels, modes1):\n","        super(SpectralConv1d, self).__init__()\n","\n","        \"\"\"\n","        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.\n","        \"\"\"\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.modes1 = modes1\n","\n","        self.scale = (1 / (in_channels * out_channels))\n","        self.weights1 = nn.Parameter(self.scale * torch.rand(in_channels, out_channels, self.modes1, dtype=torch.cfloat))\n","\n","    # Complex multiplication\n","    def compl_mul1d(self, input, weights):\n","        # (batch, in_channel, x ), (in_channel, out_channel, x) -> (batch, out_channel, x)\n","        return torch.einsum(\"bix,iox->box\", input, weights)\n","\n","    def forward(self, x):\n","        batchsize = x.shape[0]\n","        # x.shape == [batch_size, in_channels, number of grid points]\n","        # hint: use torch.fft library torch.fft.rfft\n","        # use DFT to approximate the fourier transform\n","\n","        # Compute Fourier coefficients\n","        x_ft = torch.fft.rfft(x)\n","\n","        # Multiply relevant Fourier modes\n","        out_ft = torch.zeros(batchsize, self.out_channels, x.size(-1) // 2 + 1, device=x.device, dtype=torch.cfloat)\n","        out_ft[:, :, :self.modes1] = self.compl_mul1d(x_ft[:, :, :self.modes1], self.weights1)\n","\n","        # Return to physical space\n","        x = torch.fft.irfft(out_ft, n=x.size(-1))\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j4-qeeymzooA"},"outputs":[],"source":["torch.manual_seed(0)\n","np.random.seed(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yH5u906CIx_z"},"outputs":[],"source":["class FNO1d(nn.Module):\n","    def __init__(self, modes, width):\n","        super(FNO1d, self).__init__()\n","\n","        self.modes1 = modes\n","        self.width = width\n","        self.padding = 1  # pad the domain if input is non-periodic\n","        self.linear_p = nn.Linear(2, self.width)  # input channel is 2: (u0(x), x) --> GRID IS INCLUDED!\n","\n","        self.spect1 = SpectralConv1d(self.width, self.width, self.modes1)\n","        self.spect2 = SpectralConv1d(self.width, self.width, self.modes1)\n","        self.spect3 = SpectralConv1d(self.width, self.width, self.modes1)\n","\n","        self.lin0 = nn.Conv1d(self.width, self.width, 1)\n","        self.lin1 = nn.Conv1d(self.width, self.width, 1)\n","        self.lin2 = nn.Conv1d(self.width, self.width, 1)\n","\n","        self.linear_q = nn.Linear(self.width, 100)\n","        self.output_layer = nn.Linear(100, 1)\n","\n","        self.activation = torch.nn.GELU()\n","\n","    def get_grid(self,x):\n","        res=x.shape[1]\n","        grid=torch.linspace(0,1,res)\n","        temporary_tensor=torch.zeros(x.shape[0], x.shape[1], 2)\n","        temporary_tensor[:,:,0]=x.squeeze()\n","        for i in range(x.shape[0]):\n","          temporary_tensor[i,:,1]=grid\n","        return temporary_tensor\n","\n","    def fourier_layer(self, x, spectral_layer, conv_layer):\n","        return self.activation(spectral_layer(x) + conv_layer(x))\n","\n","    def linear_layer(self, x, linear_transformation):\n","        return self.activation(linear_transformation(x))\n","\n","    def forward(self, x):\n","        x = self.get_grid(x)\n","        x = self.linear_p(x)\n","\n","        x = x.permute(0, 2, 1)\n","\n","        x = self.fourier_layer(x, self.spect1, self.lin0)\n","        x = self.fourier_layer(x, self.spect2, self.lin1)\n","        x = self.fourier_layer(x, self.spect3, self.lin2)\n","\n","        x = x.permute(0, 2, 1)\n","\n","        x = self.linear_layer(x, self.linear_q)\n","\n","        x = self.output_layer(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"yHcSuzMYGz34"},"source":["Data preproccessing:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PKpOJYdbSIe_"},"outputs":[],"source":["n=64  #selecting the train and test trajectories\n","x_data=torch.from_numpy(np.load('train_sol.npy')).type(torch.float32)[:,0,:] #trajectories at time t=0 (128x64)\n","y_data=torch.from_numpy(np.load('train_sol.npy')).type(torch.float32)[:,4,:] #trajectories at time t=1 (128x64)\n","\n","\n","input_train=x_data[:n,:] #first 64 as training at time t=0 -> initial conditions 64x64x2(traj,res,function and grid)\n","output_train=y_data[:n,:] #sol at time t=1 -> final conditions\n","input_test=x_data[n:,:]  #last 64 as validation\n","output_test=y_data[n:,:]\n","\n","batch_size=16\n","\n","training_set=DataLoader(TensorDataset(input_train,output_train),batch_size=batch_size,shuffle=True)\n","testing_set=DataLoader(TensorDataset(input_test,output_test),batch_size=batch_size,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"11mxfaTLyoOK"},"outputs":[],"source":["learning_rate = 0.001\n","epochs= 500\n","step_size = 50\n","gamma = 0.50"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLzBpRf-yrhC"},"outputs":[],"source":["modes = 16\n","width = 64\n","fno = FNO1d(modes, width) # model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zNzTaaWvNsvT"},"outputs":[],"source":["def l2loss(output_pred,real_output):\n","  return (torch.mean((output_pred - real_output) ** 2) / torch.mean(real_output ** 2)) ** 0.5 * 100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79326,"status":"ok","timestamp":1735305177277,"user":{"displayName":"ghets 79","userId":"11905067178965594936"},"user_tz":-60},"id":"Iv9PKTQjwro8","outputId":"da2b9b82-6fe2-4bc3-cf54-bf9cb9646b44"},"outputs":[{"output_type":"stream","name":"stdout","text":["######### Epoch: 0  ######### Train Loss: 98.76697540283203  ######### Relative L2 Test Norm: 96.20621681213379\n","######### Epoch: 1  ######### Train Loss: 93.72891426086426  ######### Relative L2 Test Norm: 85.87048530578613\n","######### Epoch: 2  ######### Train Loss: 82.99407768249512  ######### Relative L2 Test Norm: 77.85026741027832\n","######### Epoch: 3  ######### Train Loss: 80.04380226135254  ######### Relative L2 Test Norm: 75.16288757324219\n","######### Epoch: 4  ######### Train Loss: 74.71646881103516  ######### Relative L2 Test Norm: 75.23087310791016\n","######### Epoch: 5  ######### Train Loss: 75.28898429870605  ######### Relative L2 Test Norm: 72.4518051147461\n","######### Epoch: 6  ######### Train Loss: 70.11868476867676  ######### Relative L2 Test Norm: 67.64945983886719\n","######### Epoch: 7  ######### Train Loss: 65.94647121429443  ######### Relative L2 Test Norm: 63.44387912750244\n","######### Epoch: 8  ######### Train Loss: 57.879767417907715  ######### Relative L2 Test Norm: 55.26319694519043\n","######### Epoch: 9  ######### Train Loss: 47.59130001068115  ######### Relative L2 Test Norm: 43.27676868438721\n","######### Epoch: 10  ######### Train Loss: 39.99060344696045  ######### Relative L2 Test Norm: 38.183292388916016\n","######### Epoch: 11  ######### Train Loss: 39.25796127319336  ######### Relative L2 Test Norm: 35.02173471450806\n","######### Epoch: 12  ######### Train Loss: 33.26973485946655  ######### Relative L2 Test Norm: 33.39021444320679\n","######### Epoch: 13  ######### Train Loss: 32.37647771835327  ######### Relative L2 Test Norm: 32.15817880630493\n","######### Epoch: 14  ######### Train Loss: 31.41352367401123  ######### Relative L2 Test Norm: 31.98610830307007\n","######### Epoch: 15  ######### Train Loss: 30.16048288345337  ######### Relative L2 Test Norm: 34.609145164489746\n","######### Epoch: 16  ######### Train Loss: 29.05768346786499  ######### Relative L2 Test Norm: 32.927910804748535\n","######### Epoch: 17  ######### Train Loss: 28.46782922744751  ######### Relative L2 Test Norm: 27.801143169403076\n","######### Epoch: 18  ######### Train Loss: 24.79081439971924  ######### Relative L2 Test Norm: 27.65190362930298\n","######### Epoch: 19  ######### Train Loss: 24.544321537017822  ######### Relative L2 Test Norm: 27.402769088745117\n","######### Epoch: 20  ######### Train Loss: 22.685667514801025  ######### Relative L2 Test Norm: 25.02088737487793\n","######### Epoch: 21  ######### Train Loss: 21.702876091003418  ######### Relative L2 Test Norm: 24.2278094291687\n","######### Epoch: 22  ######### Train Loss: 20.77096700668335  ######### Relative L2 Test Norm: 22.067214488983154\n","######### Epoch: 23  ######### Train Loss: 19.555935859680176  ######### Relative L2 Test Norm: 23.003835201263428\n","######### Epoch: 24  ######### Train Loss: 18.890602111816406  ######### Relative L2 Test Norm: 21.90553331375122\n","######### Epoch: 25  ######### Train Loss: 18.223057746887207  ######### Relative L2 Test Norm: 21.71384286880493\n","######### Epoch: 26  ######### Train Loss: 18.376119136810303  ######### Relative L2 Test Norm: 21.302660942077637\n","######### Epoch: 27  ######### Train Loss: 18.20187211036682  ######### Relative L2 Test Norm: 18.936108112335205\n","######### Epoch: 28  ######### Train Loss: 16.51724910736084  ######### Relative L2 Test Norm: 21.05629587173462\n","######### Epoch: 29  ######### Train Loss: 17.061916828155518  ######### Relative L2 Test Norm: 19.69772481918335\n","######### Epoch: 30  ######### Train Loss: 15.36971402168274  ######### Relative L2 Test Norm: 16.64470911026001\n","######### Epoch: 31  ######### Train Loss: 13.850263595581055  ######### Relative L2 Test Norm: 16.984121799468994\n","######### Epoch: 32  ######### Train Loss: 12.783393859863281  ######### Relative L2 Test Norm: 16.189048051834106\n","######### Epoch: 33  ######### Train Loss: 11.66240668296814  ######### Relative L2 Test Norm: 15.733177423477173\n","######### Epoch: 34  ######### Train Loss: 10.818176031112671  ######### Relative L2 Test Norm: 13.860784530639648\n","######### Epoch: 35  ######### Train Loss: 10.269666194915771  ######### Relative L2 Test Norm: 13.79918622970581\n","######### Epoch: 36  ######### Train Loss: 10.298514366149902  ######### Relative L2 Test Norm: 13.095804452896118\n","######### Epoch: 37  ######### Train Loss: 9.54242753982544  ######### Relative L2 Test Norm: 12.645763158798218\n","######### Epoch: 38  ######### Train Loss: 9.657341957092285  ######### Relative L2 Test Norm: 13.259675741195679\n","######### Epoch: 39  ######### Train Loss: 10.19457459449768  ######### Relative L2 Test Norm: 12.073376655578613\n","######### Epoch: 40  ######### Train Loss: 9.413576602935791  ######### Relative L2 Test Norm: 12.41734504699707\n","######### Epoch: 41  ######### Train Loss: 8.823206424713135  ######### Relative L2 Test Norm: 12.445657968521118\n","######### Epoch: 42  ######### Train Loss: 8.788648128509521  ######### Relative L2 Test Norm: 11.651044845581055\n","######### Epoch: 43  ######### Train Loss: 8.375896692276001  ######### Relative L2 Test Norm: 11.327555418014526\n","######### Epoch: 44  ######### Train Loss: 8.834704399108887  ######### Relative L2 Test Norm: 10.639002799987793\n","######### Epoch: 45  ######### Train Loss: 8.987436294555664  ######### Relative L2 Test Norm: 11.158336639404297\n","######### Epoch: 46  ######### Train Loss: 8.32433533668518  ######### Relative L2 Test Norm: 11.586252927780151\n","######### Epoch: 47  ######### Train Loss: 8.203976392745972  ######### Relative L2 Test Norm: 10.729163885116577\n","######### Epoch: 48  ######### Train Loss: 7.771921753883362  ######### Relative L2 Test Norm: 10.52156925201416\n","######### Epoch: 49  ######### Train Loss: 7.77375853061676  ######### Relative L2 Test Norm: 10.14373230934143\n","######### Epoch: 50  ######### Train Loss: 6.759727954864502  ######### Relative L2 Test Norm: 9.302523851394653\n","######### Epoch: 51  ######### Train Loss: 6.3770270347595215  ######### Relative L2 Test Norm: 9.63223123550415\n","######### Epoch: 52  ######### Train Loss: 6.169491291046143  ######### Relative L2 Test Norm: 9.417722225189209\n","######### Epoch: 53  ######### Train Loss: 5.957239866256714  ######### Relative L2 Test Norm: 9.046408414840698\n","######### Epoch: 54  ######### Train Loss: 5.766143798828125  ######### Relative L2 Test Norm: 9.03419542312622\n","######### Epoch: 55  ######### Train Loss: 5.59724748134613  ######### Relative L2 Test Norm: 9.114059209823608\n","######### Epoch: 56  ######### Train Loss: 5.438906192779541  ######### Relative L2 Test Norm: 8.98887324333191\n","######### Epoch: 57  ######### Train Loss: 5.47948145866394  ######### Relative L2 Test Norm: 8.921136617660522\n","######### Epoch: 58  ######### Train Loss: 5.515036582946777  ######### Relative L2 Test Norm: 8.905742406845093\n","######### Epoch: 59  ######### Train Loss: 5.597676038742065  ######### Relative L2 Test Norm: 8.580723762512207\n","######### Epoch: 60  ######### Train Loss: 5.432129859924316  ######### Relative L2 Test Norm: 8.600508689880371\n","######### Epoch: 61  ######### Train Loss: 5.230461001396179  ######### Relative L2 Test Norm: 8.653228640556335\n","######### Epoch: 62  ######### Train Loss: 5.26587975025177  ######### Relative L2 Test Norm: 8.427301049232483\n","######### Epoch: 63  ######### Train Loss: 5.223321795463562  ######### Relative L2 Test Norm: 8.416897296905518\n","######### Epoch: 64  ######### Train Loss: 5.041916370391846  ######### Relative L2 Test Norm: 8.309607982635498\n","######### Epoch: 65  ######### Train Loss: 5.2602691650390625  ######### Relative L2 Test Norm: 8.412309646606445\n","######### Epoch: 66  ######### Train Loss: 5.435325741767883  ######### Relative L2 Test Norm: 8.372946858406067\n","######### Epoch: 67  ######### Train Loss: 5.2938631772994995  ######### Relative L2 Test Norm: 8.332919597625732\n","######### Epoch: 68  ######### Train Loss: 4.891561269760132  ######### Relative L2 Test Norm: 7.908313274383545\n","######### Epoch: 69  ######### Train Loss: 4.657530307769775  ######### Relative L2 Test Norm: 7.971697211265564\n","######### Epoch: 70  ######### Train Loss: 4.609647750854492  ######### Relative L2 Test Norm: 7.576580047607422\n","######### Epoch: 71  ######### Train Loss: 4.556373953819275  ######### Relative L2 Test Norm: 7.91011369228363\n","######### Epoch: 72  ######### Train Loss: 4.489477515220642  ######### Relative L2 Test Norm: 7.8800917863845825\n","######### Epoch: 73  ######### Train Loss: 4.704945683479309  ######### Relative L2 Test Norm: 7.476972818374634\n","######### Epoch: 74  ######### Train Loss: 4.58539342880249  ######### Relative L2 Test Norm: 7.604588747024536\n","######### Epoch: 75  ######### Train Loss: 4.648389101028442  ######### Relative L2 Test Norm: 8.014894366264343\n","######### Epoch: 76  ######### Train Loss: 4.553085565567017  ######### Relative L2 Test Norm: 7.747624635696411\n","######### Epoch: 77  ######### Train Loss: 4.531951427459717  ######### Relative L2 Test Norm: 7.178735136985779\n","######### Epoch: 78  ######### Train Loss: 4.331903517246246  ######### Relative L2 Test Norm: 7.342149496078491\n","######### Epoch: 79  ######### Train Loss: 4.564493894577026  ######### Relative L2 Test Norm: 7.776676416397095\n","######### Epoch: 80  ######### Train Loss: 4.6694406270980835  ######### Relative L2 Test Norm: 7.507458925247192\n","######### Epoch: 81  ######### Train Loss: 4.562674462795258  ######### Relative L2 Test Norm: 7.733991861343384\n","######### Epoch: 82  ######### Train Loss: 4.490943789482117  ######### Relative L2 Test Norm: 7.54404091835022\n","######### Epoch: 83  ######### Train Loss: 4.923093318939209  ######### Relative L2 Test Norm: 7.763009667396545\n","######### Epoch: 84  ######### Train Loss: 4.348101258277893  ######### Relative L2 Test Norm: 6.94614052772522\n","######### Epoch: 85  ######### Train Loss: 4.154000878334045  ######### Relative L2 Test Norm: 6.867626190185547\n","######### Epoch: 86  ######### Train Loss: 4.534583508968353  ######### Relative L2 Test Norm: 7.81131637096405\n","######### Epoch: 87  ######### Train Loss: 4.71549654006958  ######### Relative L2 Test Norm: 8.03167474269867\n","######### Epoch: 88  ######### Train Loss: 5.355679273605347  ######### Relative L2 Test Norm: 7.979274868965149\n","######### Epoch: 89  ######### Train Loss: 5.248794198036194  ######### Relative L2 Test Norm: 7.642188668251038\n","######### Epoch: 90  ######### Train Loss: 5.092531681060791  ######### Relative L2 Test Norm: 8.301981449127197\n","######### Epoch: 91  ######### Train Loss: 4.8624478578567505  ######### Relative L2 Test Norm: 7.291372895240784\n","######### Epoch: 92  ######### Train Loss: 4.7805335521698  ######### Relative L2 Test Norm: 7.264527916908264\n","######### Epoch: 93  ######### Train Loss: 4.763312458992004  ######### Relative L2 Test Norm: 7.120796203613281\n","######### Epoch: 94  ######### Train Loss: 4.683814287185669  ######### Relative L2 Test Norm: 8.132514953613281\n","######### Epoch: 95  ######### Train Loss: 5.090302467346191  ######### Relative L2 Test Norm: 7.6303499937057495\n","######### Epoch: 96  ######### Train Loss: 5.464155554771423  ######### Relative L2 Test Norm: 8.390595555305481\n","######### Epoch: 97  ######### Train Loss: 5.0190123319625854  ######### Relative L2 Test Norm: 7.1758131980896\n","######### Epoch: 98  ######### Train Loss: 5.017638802528381  ######### Relative L2 Test Norm: 7.293546557426453\n","######### Epoch: 99  ######### Train Loss: 4.281467258930206  ######### Relative L2 Test Norm: 6.914793372154236\n","######### Epoch: 100  ######### Train Loss: 3.8585221767425537  ######### Relative L2 Test Norm: 7.040645956993103\n","######### Epoch: 101  ######### Train Loss: 3.727862775325775  ######### Relative L2 Test Norm: 6.360044598579407\n","######### Epoch: 102  ######### Train Loss: 3.498505473136902  ######### Relative L2 Test Norm: 6.22940468788147\n","######### Epoch: 103  ######### Train Loss: 3.243990659713745  ######### Relative L2 Test Norm: 6.350434899330139\n","######### Epoch: 104  ######### Train Loss: 3.0555591583251953  ######### Relative L2 Test Norm: 6.191573262214661\n","######### Epoch: 105  ######### Train Loss: 3.03271746635437  ######### Relative L2 Test Norm: 6.142618656158447\n","######### Epoch: 106  ######### Train Loss: 2.9997981786727905  ######### Relative L2 Test Norm: 6.101555824279785\n","######### Epoch: 107  ######### Train Loss: 3.011649012565613  ######### Relative L2 Test Norm: 6.0241488218307495\n","######### Epoch: 108  ######### Train Loss: 2.9798880219459534  ######### Relative L2 Test Norm: 6.028735637664795\n","######### Epoch: 109  ######### Train Loss: 2.9722864627838135  ######### Relative L2 Test Norm: 6.021064281463623\n","######### Epoch: 110  ######### Train Loss: 2.977297842502594  ######### Relative L2 Test Norm: 6.019515872001648\n","######### Epoch: 111  ######### Train Loss: 2.9123483300209045  ######### Relative L2 Test Norm: 6.0934916734695435\n","######### Epoch: 112  ######### Train Loss: 2.901054799556732  ######### Relative L2 Test Norm: 5.924518704414368\n","######### Epoch: 113  ######### Train Loss: 2.9964707493782043  ######### Relative L2 Test Norm: 5.9662744998931885\n","######### Epoch: 114  ######### Train Loss: 2.8400710821151733  ######### Relative L2 Test Norm: 6.007214069366455\n","######### Epoch: 115  ######### Train Loss: 2.8056825399398804  ######### Relative L2 Test Norm: 5.813758611679077\n","######### Epoch: 116  ######### Train Loss: 2.7809146642684937  ######### Relative L2 Test Norm: 5.858120799064636\n","######### Epoch: 117  ######### Train Loss: 2.8225565552711487  ######### Relative L2 Test Norm: 5.732123136520386\n","######### Epoch: 118  ######### Train Loss: 2.796933948993683  ######### Relative L2 Test Norm: 5.938537240028381\n","######### Epoch: 119  ######### Train Loss: 2.7480397820472717  ######### Relative L2 Test Norm: 5.748790383338928\n","######### Epoch: 120  ######### Train Loss: 2.7853304743766785  ######### Relative L2 Test Norm: 5.857728958129883\n","######### Epoch: 121  ######### Train Loss: 2.7553268671035767  ######### Relative L2 Test Norm: 5.739112615585327\n","######### Epoch: 122  ######### Train Loss: 2.766649544239044  ######### Relative L2 Test Norm: 5.887469172477722\n","######### Epoch: 123  ######### Train Loss: 2.8308836817741394  ######### Relative L2 Test Norm: 5.614035129547119\n","######### Epoch: 124  ######### Train Loss: 2.731701076030731  ######### Relative L2 Test Norm: 5.816324234008789\n","######### Epoch: 125  ######### Train Loss: 2.732654094696045  ######### Relative L2 Test Norm: 5.651121020317078\n","######### Epoch: 126  ######### Train Loss: 2.6832189559936523  ######### Relative L2 Test Norm: 5.623425006866455\n","######### Epoch: 127  ######### Train Loss: 2.6323858499526978  ######### Relative L2 Test Norm: 5.6003159284591675\n","######### Epoch: 128  ######### Train Loss: 2.5788281559944153  ######### Relative L2 Test Norm: 5.507046222686768\n","######### Epoch: 129  ######### Train Loss: 2.565065026283264  ######### Relative L2 Test Norm: 5.69805908203125\n","######### Epoch: 130  ######### Train Loss: 2.670572578907013  ######### Relative L2 Test Norm: 5.630443215370178\n","######### Epoch: 131  ######### Train Loss: 2.721658766269684  ######### Relative L2 Test Norm: 5.474241495132446\n","######### Epoch: 132  ######### Train Loss: 2.7032166719436646  ######### Relative L2 Test Norm: 5.709980010986328\n","######### Epoch: 133  ######### Train Loss: 2.773796498775482  ######### Relative L2 Test Norm: 5.463771462440491\n","######### Epoch: 134  ######### Train Loss: 2.6684237122535706  ######### Relative L2 Test Norm: 5.573109984397888\n","######### Epoch: 135  ######### Train Loss: 2.7171754240989685  ######### Relative L2 Test Norm: 5.454533100128174\n","######### Epoch: 136  ######### Train Loss: 2.75228089094162  ######### Relative L2 Test Norm: 5.3997498750686646\n","######### Epoch: 137  ######### Train Loss: 2.648172676563263  ######### Relative L2 Test Norm: 5.45665180683136\n","######### Epoch: 138  ######### Train Loss: 2.780356466770172  ######### Relative L2 Test Norm: 5.402184724807739\n","######### Epoch: 139  ######### Train Loss: 2.595313549041748  ######### Relative L2 Test Norm: 5.434191823005676\n","######### Epoch: 140  ######### Train Loss: 2.5294072031974792  ######### Relative L2 Test Norm: 5.405962586402893\n","######### Epoch: 141  ######### Train Loss: 2.664405405521393  ######### Relative L2 Test Norm: 5.382753133773804\n","######### Epoch: 142  ######### Train Loss: 2.6073323488235474  ######### Relative L2 Test Norm: 5.311359524726868\n","######### Epoch: 143  ######### Train Loss: 2.7389893531799316  ######### Relative L2 Test Norm: 5.354935646057129\n","######### Epoch: 144  ######### Train Loss: 2.5085846185684204  ######### Relative L2 Test Norm: 5.435732126235962\n","######### Epoch: 145  ######### Train Loss: 2.5797548294067383  ######### Relative L2 Test Norm: 5.342791318893433\n","######### Epoch: 146  ######### Train Loss: 2.4748311638832092  ######### Relative L2 Test Norm: 5.147053003311157\n","######### Epoch: 147  ######### Train Loss: 2.7393882274627686  ######### Relative L2 Test Norm: 5.449736714363098\n","######### Epoch: 148  ######### Train Loss: 2.6247037649154663  ######### Relative L2 Test Norm: 5.241063714027405\n","######### Epoch: 149  ######### Train Loss: 2.6246318221092224  ######### Relative L2 Test Norm: 5.658570766448975\n","######### Epoch: 150  ######### Train Loss: 2.6100558638572693  ######### Relative L2 Test Norm: 5.015928149223328\n","######### Epoch: 151  ######### Train Loss: 2.494069516658783  ######### Relative L2 Test Norm: 5.198772668838501\n","######### Epoch: 152  ######### Train Loss: 2.3550968766212463  ######### Relative L2 Test Norm: 5.057844161987305\n","######### Epoch: 153  ######### Train Loss: 2.29805451631546  ######### Relative L2 Test Norm: 5.155214905738831\n","######### Epoch: 154  ######### Train Loss: 2.274849236011505  ######### Relative L2 Test Norm: 5.071735143661499\n","######### Epoch: 155  ######### Train Loss: 2.209568291902542  ######### Relative L2 Test Norm: 5.112642884254456\n","######### Epoch: 156  ######### Train Loss: 2.1469139754772186  ######### Relative L2 Test Norm: 4.990518808364868\n","######### Epoch: 157  ######### Train Loss: 2.0933460891246796  ######### Relative L2 Test Norm: 5.097838878631592\n","######### Epoch: 158  ######### Train Loss: 2.1144649386405945  ######### Relative L2 Test Norm: 4.9889068603515625\n","######### Epoch: 159  ######### Train Loss: 2.0465103089809418  ######### Relative L2 Test Norm: 4.990991711616516\n","######### Epoch: 160  ######### Train Loss: 2.009299337863922  ######### Relative L2 Test Norm: 5.02158784866333\n","######### Epoch: 161  ######### Train Loss: 2.026662677526474  ######### Relative L2 Test Norm: 4.905053973197937\n","######### Epoch: 162  ######### Train Loss: 2.0735665559768677  ######### Relative L2 Test Norm: 4.997736692428589\n","######### Epoch: 163  ######### Train Loss: 2.0359730422496796  ######### Relative L2 Test Norm: 4.8975735902786255\n","######### Epoch: 164  ######### Train Loss: 2.0564351975917816  ######### Relative L2 Test Norm: 4.965442180633545\n","######### Epoch: 165  ######### Train Loss: 2.0451337099075317  ######### Relative L2 Test Norm: 4.89568305015564\n","######### Epoch: 166  ######### Train Loss: 2.0356627106666565  ######### Relative L2 Test Norm: 4.933563828468323\n","######### Epoch: 167  ######### Train Loss: 2.063006043434143  ######### Relative L2 Test Norm: 4.8048189878463745\n","######### Epoch: 168  ######### Train Loss: 1.999432772397995  ######### Relative L2 Test Norm: 4.942074418067932\n","######### Epoch: 169  ######### Train Loss: 1.9890123009681702  ######### Relative L2 Test Norm: 4.828836917877197\n","######### Epoch: 170  ######### Train Loss: 1.9915139973163605  ######### Relative L2 Test Norm: 4.936742424964905\n","######### Epoch: 171  ######### Train Loss: 1.9559307992458344  ######### Relative L2 Test Norm: 4.786312580108643\n","######### Epoch: 172  ######### Train Loss: 1.9698775112628937  ######### Relative L2 Test Norm: 4.787030458450317\n","######### Epoch: 173  ######### Train Loss: 1.9907945692539215  ######### Relative L2 Test Norm: 4.840063571929932\n","######### Epoch: 174  ######### Train Loss: 1.968980610370636  ######### Relative L2 Test Norm: 4.792534708976746\n","######### Epoch: 175  ######### Train Loss: 1.9694305658340454  ######### Relative L2 Test Norm: 4.8045525550842285\n","######### Epoch: 176  ######### Train Loss: 1.9252821505069733  ######### Relative L2 Test Norm: 4.776249885559082\n","######### Epoch: 177  ######### Train Loss: 1.9225435256958008  ######### Relative L2 Test Norm: 4.772032976150513\n","######### Epoch: 178  ######### Train Loss: 1.965818703174591  ######### Relative L2 Test Norm: 4.768197536468506\n","######### Epoch: 179  ######### Train Loss: 1.919663429260254  ######### Relative L2 Test Norm: 4.795395731925964\n","######### Epoch: 180  ######### Train Loss: 1.9485990703105927  ######### Relative L2 Test Norm: 4.654135346412659\n","######### Epoch: 181  ######### Train Loss: 1.9479082524776459  ######### Relative L2 Test Norm: 4.807112812995911\n","######### Epoch: 182  ######### Train Loss: 1.9417162835597992  ######### Relative L2 Test Norm: 4.741593956947327\n","######### Epoch: 183  ######### Train Loss: 1.9773876368999481  ######### Relative L2 Test Norm: 4.741949558258057\n","######### Epoch: 184  ######### Train Loss: 2.000944584608078  ######### Relative L2 Test Norm: 4.748412489891052\n","######### Epoch: 185  ######### Train Loss: 2.0241843461990356  ######### Relative L2 Test Norm: 4.773157119750977\n","######### Epoch: 186  ######### Train Loss: 2.029593378305435  ######### Relative L2 Test Norm: 4.668954610824585\n","######### Epoch: 187  ######### Train Loss: 1.9568881392478943  ######### Relative L2 Test Norm: 4.7267632484436035\n","######### Epoch: 188  ######### Train Loss: 1.9307787120342255  ######### Relative L2 Test Norm: 4.701038241386414\n","######### Epoch: 189  ######### Train Loss: 1.8960034251213074  ######### Relative L2 Test Norm: 4.73242974281311\n","######### Epoch: 190  ######### Train Loss: 1.8658619821071625  ######### Relative L2 Test Norm: 4.615067005157471\n","######### Epoch: 191  ######### Train Loss: 1.8750873804092407  ######### Relative L2 Test Norm: 4.658802628517151\n","######### Epoch: 192  ######### Train Loss: 1.8472008109092712  ######### Relative L2 Test Norm: 4.697031021118164\n","######### Epoch: 193  ######### Train Loss: 1.8565170764923096  ######### Relative L2 Test Norm: 4.656625151634216\n","######### Epoch: 194  ######### Train Loss: 1.8504983186721802  ######### Relative L2 Test Norm: 4.621504068374634\n","######### Epoch: 195  ######### Train Loss: 1.824534296989441  ######### Relative L2 Test Norm: 4.605372071266174\n","######### Epoch: 196  ######### Train Loss: 1.8078052401542664  ######### Relative L2 Test Norm: 4.6729289293289185\n","######### Epoch: 197  ######### Train Loss: 1.7952929735183716  ######### Relative L2 Test Norm: 4.590364575386047\n","######### Epoch: 198  ######### Train Loss: 1.8231011629104614  ######### Relative L2 Test Norm: 4.590232253074646\n","######### Epoch: 199  ######### Train Loss: 1.8441242277622223  ######### Relative L2 Test Norm: 4.723551392555237\n","######### Epoch: 200  ######### Train Loss: 1.876490294933319  ######### Relative L2 Test Norm: 4.496125161647797\n","######### Epoch: 201  ######### Train Loss: 1.8085987567901611  ######### Relative L2 Test Norm: 4.580357909202576\n","######### Epoch: 202  ######### Train Loss: 1.7571503818035126  ######### Relative L2 Test Norm: 4.591559886932373\n","######### Epoch: 203  ######### Train Loss: 1.7864857614040375  ######### Relative L2 Test Norm: 4.547425627708435\n","######### Epoch: 204  ######### Train Loss: 1.7690466046333313  ######### Relative L2 Test Norm: 4.534972190856934\n","######### Epoch: 205  ######### Train Loss: 1.7742812037467957  ######### Relative L2 Test Norm: 4.534411311149597\n","######### Epoch: 206  ######### Train Loss: 1.7801803648471832  ######### Relative L2 Test Norm: 4.477692008018494\n","######### Epoch: 207  ######### Train Loss: 1.7683579325675964  ######### Relative L2 Test Norm: 4.576192498207092\n","######### Epoch: 208  ######### Train Loss: 1.7702276110649109  ######### Relative L2 Test Norm: 4.4883140325546265\n","######### Epoch: 209  ######### Train Loss: 1.761025756597519  ######### Relative L2 Test Norm: 4.570480823516846\n","######### Epoch: 210  ######### Train Loss: 1.761517971754074  ######### Relative L2 Test Norm: 4.501545310020447\n","######### Epoch: 211  ######### Train Loss: 1.718207448720932  ######### Relative L2 Test Norm: 4.565998077392578\n","######### Epoch: 212  ######### Train Loss: 1.720964640378952  ######### Relative L2 Test Norm: 4.474387168884277\n","######### Epoch: 213  ######### Train Loss: 1.6996293663978577  ######### Relative L2 Test Norm: 4.515142321586609\n","######### Epoch: 214  ######### Train Loss: 1.692852407693863  ######### Relative L2 Test Norm: 4.50560736656189\n","######### Epoch: 215  ######### Train Loss: 1.6975956857204437  ######### Relative L2 Test Norm: 4.493617296218872\n","######### Epoch: 216  ######### Train Loss: 1.6894647777080536  ######### Relative L2 Test Norm: 4.4513245820999146\n","######### Epoch: 217  ######### Train Loss: 1.6902086734771729  ######### Relative L2 Test Norm: 4.426164507865906\n","######### Epoch: 218  ######### Train Loss: 1.6750614643096924  ######### Relative L2 Test Norm: 4.487066388130188\n","######### Epoch: 219  ######### Train Loss: 1.7064954340457916  ######### Relative L2 Test Norm: 4.494728624820709\n","######### Epoch: 220  ######### Train Loss: 1.673881322145462  ######### Relative L2 Test Norm: 4.450092196464539\n","######### Epoch: 221  ######### Train Loss: 1.673671156167984  ######### Relative L2 Test Norm: 4.439717531204224\n","######### Epoch: 222  ######### Train Loss: 1.6805006265640259  ######### Relative L2 Test Norm: 4.434323608875275\n","######### Epoch: 223  ######### Train Loss: 1.7023445665836334  ######### Relative L2 Test Norm: 4.494458556175232\n","######### Epoch: 224  ######### Train Loss: 1.7125348150730133  ######### Relative L2 Test Norm: 4.399580717086792\n","######### Epoch: 225  ######### Train Loss: 1.6661242842674255  ######### Relative L2 Test Norm: 4.477634072303772\n","######### Epoch: 226  ######### Train Loss: 1.6677262485027313  ######### Relative L2 Test Norm: 4.454008936882019\n","######### Epoch: 227  ######### Train Loss: 1.6894115209579468  ######### Relative L2 Test Norm: 4.462158679962158\n","######### Epoch: 228  ######### Train Loss: 1.67905655503273  ######### Relative L2 Test Norm: 4.499101996421814\n","######### Epoch: 229  ######### Train Loss: 1.6678122878074646  ######### Relative L2 Test Norm: 4.411400198936462\n","######### Epoch: 230  ######### Train Loss: 1.6871935725212097  ######### Relative L2 Test Norm: 4.4251629114151\n","######### Epoch: 231  ######### Train Loss: 1.6474952697753906  ######### Relative L2 Test Norm: 4.38828718662262\n","######### Epoch: 232  ######### Train Loss: 1.6690613627433777  ######### Relative L2 Test Norm: 4.376003265380859\n","######### Epoch: 233  ######### Train Loss: 1.6517005860805511  ######### Relative L2 Test Norm: 4.414755344390869\n","######### Epoch: 234  ######### Train Loss: 1.651893824338913  ######### Relative L2 Test Norm: 4.42031455039978\n","######### Epoch: 235  ######### Train Loss: 1.6631464064121246  ######### Relative L2 Test Norm: 4.387811779975891\n","######### Epoch: 236  ######### Train Loss: 1.6336773037910461  ######### Relative L2 Test Norm: 4.353725075721741\n","######### Epoch: 237  ######### Train Loss: 1.6231876909732819  ######### Relative L2 Test Norm: 4.393656015396118\n","######### Epoch: 238  ######### Train Loss: 1.626196414232254  ######### Relative L2 Test Norm: 4.397382736206055\n","######### Epoch: 239  ######### Train Loss: 1.6383224725723267  ######### Relative L2 Test Norm: 4.40065598487854\n","######### Epoch: 240  ######### Train Loss: 1.638893961906433  ######### Relative L2 Test Norm: 4.387009859085083\n","######### Epoch: 241  ######### Train Loss: 1.6318922638893127  ######### Relative L2 Test Norm: 4.3222222328186035\n","######### Epoch: 242  ######### Train Loss: 1.6827356815338135  ######### Relative L2 Test Norm: 4.385531306266785\n","######### Epoch: 243  ######### Train Loss: 1.6398311257362366  ######### Relative L2 Test Norm: 4.332146167755127\n","######### Epoch: 244  ######### Train Loss: 1.6569814085960388  ######### Relative L2 Test Norm: 4.365709543228149\n","######### Epoch: 245  ######### Train Loss: 1.6100174486637115  ######### Relative L2 Test Norm: 4.33103084564209\n","######### Epoch: 246  ######### Train Loss: 1.6181526184082031  ######### Relative L2 Test Norm: 4.381388425827026\n","######### Epoch: 247  ######### Train Loss: 1.5999684929847717  ######### Relative L2 Test Norm: 4.3251001834869385\n","######### Epoch: 248  ######### Train Loss: 1.6230891942977905  ######### Relative L2 Test Norm: 4.355455160140991\n","######### Epoch: 249  ######### Train Loss: 1.5995839536190033  ######### Relative L2 Test Norm: 4.33877956867218\n","######### Epoch: 250  ######### Train Loss: 1.5769956707954407  ######### Relative L2 Test Norm: 4.3282310962677\n","######### Epoch: 251  ######### Train Loss: 1.5701427459716797  ######### Relative L2 Test Norm: 4.345567524433136\n","######### Epoch: 252  ######### Train Loss: 1.5604941248893738  ######### Relative L2 Test Norm: 4.329056620597839\n","######### Epoch: 253  ######### Train Loss: 1.557633638381958  ######### Relative L2 Test Norm: 4.348436594009399\n","######### Epoch: 254  ######### Train Loss: 1.5657304227352142  ######### Relative L2 Test Norm: 4.314677715301514\n","######### Epoch: 255  ######### Train Loss: 1.5684228837490082  ######### Relative L2 Test Norm: 4.3119757771492\n","######### Epoch: 256  ######### Train Loss: 1.5654427111148834  ######### Relative L2 Test Norm: 4.303566038608551\n","######### Epoch: 257  ######### Train Loss: 1.5682317614555359  ######### Relative L2 Test Norm: 4.318992376327515\n","######### Epoch: 258  ######### Train Loss: 1.604295700788498  ######### Relative L2 Test Norm: 4.298005700111389\n","######### Epoch: 259  ######### Train Loss: 1.5632332861423492  ######### Relative L2 Test Norm: 4.303323149681091\n","######### Epoch: 260  ######### Train Loss: 1.5570736527442932  ######### Relative L2 Test Norm: 4.283076524734497\n","######### Epoch: 261  ######### Train Loss: 1.5583550930023193  ######### Relative L2 Test Norm: 4.334181725978851\n","######### Epoch: 262  ######### Train Loss: 1.5445893108844757  ######### Relative L2 Test Norm: 4.334434151649475\n","######### Epoch: 263  ######### Train Loss: 1.5475810170173645  ######### Relative L2 Test Norm: 4.291267037391663\n","######### Epoch: 264  ######### Train Loss: 1.5518392622470856  ######### Relative L2 Test Norm: 4.330638825893402\n","######### Epoch: 265  ######### Train Loss: 1.5558670163154602  ######### Relative L2 Test Norm: 4.282471060752869\n","######### Epoch: 266  ######### Train Loss: 1.5499021708965302  ######### Relative L2 Test Norm: 4.288681745529175\n","######### Epoch: 267  ######### Train Loss: 1.551967591047287  ######### Relative L2 Test Norm: 4.299714624881744\n","######### Epoch: 268  ######### Train Loss: 1.5372414290904999  ######### Relative L2 Test Norm: 4.265268266201019\n","######### Epoch: 269  ######### Train Loss: 1.5426456928253174  ######### Relative L2 Test Norm: 4.276175379753113\n","######### Epoch: 270  ######### Train Loss: 1.5367161333560944  ######### Relative L2 Test Norm: 4.264116406440735\n","######### Epoch: 271  ######### Train Loss: 1.5389567017555237  ######### Relative L2 Test Norm: 4.283769369125366\n","######### Epoch: 272  ######### Train Loss: 1.551996797323227  ######### Relative L2 Test Norm: 4.269034504890442\n","######### Epoch: 273  ######### Train Loss: 1.545478194952011  ######### Relative L2 Test Norm: 4.299701452255249\n","######### Epoch: 274  ######### Train Loss: 1.5447517335414886  ######### Relative L2 Test Norm: 4.259709596633911\n","######### Epoch: 275  ######### Train Loss: 1.5377139747142792  ######### Relative L2 Test Norm: 4.306794345378876\n","######### Epoch: 276  ######### Train Loss: 1.5321987569332123  ######### Relative L2 Test Norm: 4.230501651763916\n","######### Epoch: 277  ######### Train Loss: 1.5383179187774658  ######### Relative L2 Test Norm: 4.234401881694794\n","######### Epoch: 278  ######### Train Loss: 1.5314149856567383  ######### Relative L2 Test Norm: 4.293453276157379\n","######### Epoch: 279  ######### Train Loss: 1.5382206737995148  ######### Relative L2 Test Norm: 4.313902735710144\n","######### Epoch: 280  ######### Train Loss: 1.5319522023200989  ######### Relative L2 Test Norm: 4.225805342197418\n","######### Epoch: 281  ######### Train Loss: 1.5257042348384857  ######### Relative L2 Test Norm: 4.248286426067352\n","######### Epoch: 282  ######### Train Loss: 1.541326254606247  ######### Relative L2 Test Norm: 4.273292899131775\n","######### Epoch: 283  ######### Train Loss: 1.554267406463623  ######### Relative L2 Test Norm: 4.217947721481323\n","######### Epoch: 284  ######### Train Loss: 1.524274468421936  ######### Relative L2 Test Norm: 4.256758451461792\n","######### Epoch: 285  ######### Train Loss: 1.5250388085842133  ######### Relative L2 Test Norm: 4.285217523574829\n","######### Epoch: 286  ######### Train Loss: 1.5339129269123077  ######### Relative L2 Test Norm: 4.230566501617432\n","######### Epoch: 287  ######### Train Loss: 1.5267364382743835  ######### Relative L2 Test Norm: 4.226378262042999\n","######### Epoch: 288  ######### Train Loss: 1.5192123353481293  ######### Relative L2 Test Norm: 4.251351714134216\n","######### Epoch: 289  ######### Train Loss: 1.519805669784546  ######### Relative L2 Test Norm: 4.219907641410828\n","######### Epoch: 290  ######### Train Loss: 1.5198083817958832  ######### Relative L2 Test Norm: 4.2221293449401855\n","######### Epoch: 291  ######### Train Loss: 1.5104267299175262  ######### Relative L2 Test Norm: 4.234695911407471\n","######### Epoch: 292  ######### Train Loss: 1.5145065784454346  ######### Relative L2 Test Norm: 4.175883531570435\n","######### Epoch: 293  ######### Train Loss: 1.5232299268245697  ######### Relative L2 Test Norm: 4.240234613418579\n","######### Epoch: 294  ######### Train Loss: 1.5288291573524475  ######### Relative L2 Test Norm: 4.261574864387512\n","######### Epoch: 295  ######### Train Loss: 1.5210159420967102  ######### Relative L2 Test Norm: 4.172854423522949\n","######### Epoch: 296  ######### Train Loss: 1.512327253818512  ######### Relative L2 Test Norm: 4.21385657787323\n","######### Epoch: 297  ######### Train Loss: 1.5104630589485168  ######### Relative L2 Test Norm: 4.2313971519470215\n","######### Epoch: 298  ######### Train Loss: 1.514177143573761  ######### Relative L2 Test Norm: 4.222326576709747\n","######### Epoch: 299  ######### Train Loss: 1.5178539752960205  ######### Relative L2 Test Norm: 4.22212678194046\n","######### Epoch: 300  ######### Train Loss: 1.5057553350925446  ######### Relative L2 Test Norm: 4.2082366943359375\n","######### Epoch: 301  ######### Train Loss: 1.4978626370429993  ######### Relative L2 Test Norm: 4.197666347026825\n","######### Epoch: 302  ######### Train Loss: 1.493759036064148  ######### Relative L2 Test Norm: 4.221393704414368\n","######### Epoch: 303  ######### Train Loss: 1.4993550181388855  ######### Relative L2 Test Norm: 4.190975189208984\n","######### Epoch: 304  ######### Train Loss: 1.4914036095142365  ######### Relative L2 Test Norm: 4.180047988891602\n","######### Epoch: 305  ######### Train Loss: 1.4847016036510468  ######### Relative L2 Test Norm: 4.165813148021698\n","######### Epoch: 306  ######### Train Loss: 1.4836675822734833  ######### Relative L2 Test Norm: 4.221590936183929\n","######### Epoch: 307  ######### Train Loss: 1.4947825968265533  ######### Relative L2 Test Norm: 4.179877817630768\n","######### Epoch: 308  ######### Train Loss: 1.4884957373142242  ######### Relative L2 Test Norm: 4.181932687759399\n","######### Epoch: 309  ######### Train Loss: 1.4842924177646637  ######### Relative L2 Test Norm: 4.204288840293884\n","######### Epoch: 310  ######### Train Loss: 1.4896016120910645  ######### Relative L2 Test Norm: 4.199829280376434\n","######### Epoch: 311  ######### Train Loss: 1.4870099425315857  ######### Relative L2 Test Norm: 4.177252650260925\n","######### Epoch: 312  ######### Train Loss: 1.478643774986267  ######### Relative L2 Test Norm: 4.207316756248474\n","######### Epoch: 313  ######### Train Loss: 1.5104070007801056  ######### Relative L2 Test Norm: 4.213620185852051\n","######### Epoch: 314  ######### Train Loss: 1.4778464138507843  ######### Relative L2 Test Norm: 4.194111466407776\n","######### Epoch: 315  ######### Train Loss: 1.4934567213058472  ######### Relative L2 Test Norm: 4.1857317090034485\n","######### Epoch: 316  ######### Train Loss: 1.4825389385223389  ######### Relative L2 Test Norm: 4.17839789390564\n","######### Epoch: 317  ######### Train Loss: 1.4738511443138123  ######### Relative L2 Test Norm: 4.186177968978882\n","######### Epoch: 318  ######### Train Loss: 1.4810377061367035  ######### Relative L2 Test Norm: 4.217046439647675\n","######### Epoch: 319  ######### Train Loss: 1.4980666935443878  ######### Relative L2 Test Norm: 4.2091556787490845\n","######### Epoch: 320  ######### Train Loss: 1.480611890554428  ######### Relative L2 Test Norm: 4.197859168052673\n","######### Epoch: 321  ######### Train Loss: 1.475052922964096  ######### Relative L2 Test Norm: 4.147875189781189\n","######### Epoch: 322  ######### Train Loss: 1.4952380657196045  ######### Relative L2 Test Norm: 4.176025807857513\n","######### Epoch: 323  ######### Train Loss: 1.4899443089962006  ######### Relative L2 Test Norm: 4.195083677768707\n","######### Epoch: 324  ######### Train Loss: 1.4821092784404755  ######### Relative L2 Test Norm: 4.190102577209473\n","######### Epoch: 325  ######### Train Loss: 1.4784202575683594  ######### Relative L2 Test Norm: 4.182345151901245\n","######### Epoch: 326  ######### Train Loss: 1.470596045255661  ######### Relative L2 Test Norm: 4.188276410102844\n","######### Epoch: 327  ######### Train Loss: 1.4889765679836273  ######### Relative L2 Test Norm: 4.161611378192902\n","######### Epoch: 328  ######### Train Loss: 1.4709472060203552  ######### Relative L2 Test Norm: 4.184297204017639\n","######### Epoch: 329  ######### Train Loss: 1.4817794263362885  ######### Relative L2 Test Norm: 4.2140713930130005\n","######### Epoch: 330  ######### Train Loss: 1.4800733923912048  ######### Relative L2 Test Norm: 4.1635801792144775\n","######### Epoch: 331  ######### Train Loss: 1.4687312841415405  ######### Relative L2 Test Norm: 4.134656012058258\n","######### Epoch: 332  ######### Train Loss: 1.469423085451126  ######### Relative L2 Test Norm: 4.154087424278259\n","######### Epoch: 333  ######### Train Loss: 1.4880817532539368  ######### Relative L2 Test Norm: 4.177965939044952\n","######### Epoch: 334  ######### Train Loss: 1.4737339615821838  ######### Relative L2 Test Norm: 4.134763836860657\n","######### Epoch: 335  ######### Train Loss: 1.4673139452934265  ######### Relative L2 Test Norm: 4.140230417251587\n","######### Epoch: 336  ######### Train Loss: 1.4631633162498474  ######### Relative L2 Test Norm: 4.154356777667999\n","######### Epoch: 337  ######### Train Loss: 1.4647675156593323  ######### Relative L2 Test Norm: 4.161989152431488\n","######### Epoch: 338  ######### Train Loss: 1.4665205776691437  ######### Relative L2 Test Norm: 4.167721509933472\n","######### Epoch: 339  ######### Train Loss: 1.4623668789863586  ######### Relative L2 Test Norm: 4.174192130565643\n","######### Epoch: 340  ######### Train Loss: 1.4785076975822449  ######### Relative L2 Test Norm: 4.154364347457886\n","######### Epoch: 341  ######### Train Loss: 1.4769236147403717  ######### Relative L2 Test Norm: 4.14979475736618\n","######### Epoch: 342  ######### Train Loss: 1.4766645431518555  ######### Relative L2 Test Norm: 4.170603632926941\n","######### Epoch: 343  ######### Train Loss: 1.4618831276893616  ######### Relative L2 Test Norm: 4.156060516834259\n","######### Epoch: 344  ######### Train Loss: 1.463136374950409  ######### Relative L2 Test Norm: 4.153528869152069\n","######### Epoch: 345  ######### Train Loss: 1.4603165686130524  ######### Relative L2 Test Norm: 4.185816168785095\n","######### Epoch: 346  ######### Train Loss: 1.4683277606964111  ######### Relative L2 Test Norm: 4.153070569038391\n","######### Epoch: 347  ######### Train Loss: 1.467239499092102  ######### Relative L2 Test Norm: 4.188587665557861\n","######### Epoch: 348  ######### Train Loss: 1.4610464870929718  ######### Relative L2 Test Norm: 4.15993857383728\n","######### Epoch: 349  ######### Train Loss: 1.4704342186450958  ######### Relative L2 Test Norm: 4.170469224452972\n","######### Epoch: 350  ######### Train Loss: 1.4576320052146912  ######### Relative L2 Test Norm: 4.140611290931702\n","######### Epoch: 351  ######### Train Loss: 1.4553240239620209  ######### Relative L2 Test Norm: 4.113591194152832\n","######### Epoch: 352  ######### Train Loss: 1.453337848186493  ######### Relative L2 Test Norm: 4.146488428115845\n","######### Epoch: 353  ######### Train Loss: 1.4559930562973022  ######### Relative L2 Test Norm: 4.1618000864982605\n","######### Epoch: 354  ######### Train Loss: 1.4545375406742096  ######### Relative L2 Test Norm: 4.126307487487793\n","######### Epoch: 355  ######### Train Loss: 1.4607410430908203  ######### Relative L2 Test Norm: 4.133136451244354\n","######### Epoch: 356  ######### Train Loss: 1.4496304392814636  ######### Relative L2 Test Norm: 4.142448306083679\n","######### Epoch: 357  ######### Train Loss: 1.447093665599823  ######### Relative L2 Test Norm: 4.126130402088165\n","######### Epoch: 358  ######### Train Loss: 1.451636403799057  ######### Relative L2 Test Norm: 4.148399174213409\n","######### Epoch: 359  ######### Train Loss: 1.4636890590190887  ######### Relative L2 Test Norm: 4.127880930900574\n","######### Epoch: 360  ######### Train Loss: 1.4595422744750977  ######### Relative L2 Test Norm: 4.127015054225922\n","######### Epoch: 361  ######### Train Loss: 1.4600937068462372  ######### Relative L2 Test Norm: 4.1285693645477295\n","######### Epoch: 362  ######### Train Loss: 1.451655387878418  ######### Relative L2 Test Norm: 4.152946472167969\n","######### Epoch: 363  ######### Train Loss: 1.4475663304328918  ######### Relative L2 Test Norm: 4.139849126338959\n","######### Epoch: 364  ######### Train Loss: 1.454855054616928  ######### Relative L2 Test Norm: 4.1352866888046265\n","######### Epoch: 365  ######### Train Loss: 1.4626584649085999  ######### Relative L2 Test Norm: 4.125582039356232\n","######### Epoch: 366  ######### Train Loss: 1.45139542222023  ######### Relative L2 Test Norm: 4.176869094371796\n","######### Epoch: 367  ######### Train Loss: 1.4502663910388947  ######### Relative L2 Test Norm: 4.082926630973816\n","######### Epoch: 368  ######### Train Loss: 1.454140841960907  ######### Relative L2 Test Norm: 4.17546683549881\n","######### Epoch: 369  ######### Train Loss: 1.4426319599151611  ######### Relative L2 Test Norm: 4.116316258907318\n","######### Epoch: 370  ######### Train Loss: 1.4722586274147034  ######### Relative L2 Test Norm: 4.152806460857391\n","######### Epoch: 371  ######### Train Loss: 1.4465579688549042  ######### Relative L2 Test Norm: 4.164961934089661\n","######### Epoch: 372  ######### Train Loss: 1.4552358388900757  ######### Relative L2 Test Norm: 4.116885125637054\n","######### Epoch: 373  ######### Train Loss: 1.4502479434013367  ######### Relative L2 Test Norm: 4.102308392524719\n","######### Epoch: 374  ######### Train Loss: 1.4533233642578125  ######### Relative L2 Test Norm: 4.158101320266724\n","######### Epoch: 375  ######### Train Loss: 1.4462349116802216  ######### Relative L2 Test Norm: 4.128380835056305\n","######### Epoch: 376  ######### Train Loss: 1.450414776802063  ######### Relative L2 Test Norm: 4.130093693733215\n","######### Epoch: 377  ######### Train Loss: 1.4424639344215393  ######### Relative L2 Test Norm: 4.097228765487671\n","######### Epoch: 378  ######### Train Loss: 1.4394474625587463  ######### Relative L2 Test Norm: 4.115127682685852\n","######### Epoch: 379  ######### Train Loss: 1.4454641938209534  ######### Relative L2 Test Norm: 4.149705111980438\n","######### Epoch: 380  ######### Train Loss: 1.4635378420352936  ######### Relative L2 Test Norm: 4.104387819766998\n","######### Epoch: 381  ######### Train Loss: 1.4396408796310425  ######### Relative L2 Test Norm: 4.114055633544922\n","######### Epoch: 382  ######### Train Loss: 1.4562331140041351  ######### Relative L2 Test Norm: 4.149628818035126\n","######### Epoch: 383  ######### Train Loss: 1.4367387294769287  ######### Relative L2 Test Norm: 4.125895023345947\n","######### Epoch: 384  ######### Train Loss: 1.4428984224796295  ######### Relative L2 Test Norm: 4.06906133890152\n","######### Epoch: 385  ######### Train Loss: 1.440216302871704  ######### Relative L2 Test Norm: 4.117256045341492\n","######### Epoch: 386  ######### Train Loss: 1.4534644782543182  ######### Relative L2 Test Norm: 4.117588102817535\n","######### Epoch: 387  ######### Train Loss: 1.4630839228630066  ######### Relative L2 Test Norm: 4.1233726143836975\n","######### Epoch: 388  ######### Train Loss: 1.43975430727005  ######### Relative L2 Test Norm: 4.13787579536438\n","######### Epoch: 389  ######### Train Loss: 1.4402653574943542  ######### Relative L2 Test Norm: 4.158845663070679\n","######### Epoch: 390  ######### Train Loss: 1.441618412733078  ######### Relative L2 Test Norm: 4.133104681968689\n","######### Epoch: 391  ######### Train Loss: 1.441395342350006  ######### Relative L2 Test Norm: 4.092088937759399\n","######### Epoch: 392  ######### Train Loss: 1.4427395462989807  ######### Relative L2 Test Norm: 4.116257727146149\n","######### Epoch: 393  ######### Train Loss: 1.4331699013710022  ######### Relative L2 Test Norm: 4.132384717464447\n","######### Epoch: 394  ######### Train Loss: 1.4536434710025787  ######### Relative L2 Test Norm: 4.14166533946991\n","######### Epoch: 395  ######### Train Loss: 1.439549297094345  ######### Relative L2 Test Norm: 4.1146241426467896\n","######### Epoch: 396  ######### Train Loss: 1.4404577910900116  ######### Relative L2 Test Norm: 4.110891819000244\n","######### Epoch: 397  ######### Train Loss: 1.438856154680252  ######### Relative L2 Test Norm: 4.118803679943085\n","######### Epoch: 398  ######### Train Loss: 1.4434335231781006  ######### Relative L2 Test Norm: 4.092092275619507\n","######### Epoch: 399  ######### Train Loss: 1.4360934495925903  ######### Relative L2 Test Norm: 4.120948255062103\n","######### Epoch: 400  ######### Train Loss: 1.4385133385658264  ######### Relative L2 Test Norm: 4.116811037063599\n","######### Epoch: 401  ######### Train Loss: 1.4353663623332977  ######### Relative L2 Test Norm: 4.114238798618317\n","######### Epoch: 402  ######### Train Loss: 1.4363143146038055  ######### Relative L2 Test Norm: 4.129632592201233\n","######### Epoch: 403  ######### Train Loss: 1.437166303396225  ######### Relative L2 Test Norm: 4.122699677944183\n","######### Epoch: 404  ######### Train Loss: 1.432513415813446  ######### Relative L2 Test Norm: 4.119371831417084\n","######### Epoch: 405  ######### Train Loss: 1.4336465001106262  ######### Relative L2 Test Norm: 4.0808857679367065\n","######### Epoch: 406  ######### Train Loss: 1.4291675090789795  ######### Relative L2 Test Norm: 4.086061954498291\n","######### Epoch: 407  ######### Train Loss: 1.4309329390525818  ######### Relative L2 Test Norm: 4.113281190395355\n","######### Epoch: 408  ######### Train Loss: 1.429393231868744  ######### Relative L2 Test Norm: 4.110116004943848\n","######### Epoch: 409  ######### Train Loss: 1.4492061138153076  ######### Relative L2 Test Norm: 4.097132086753845\n","######### Epoch: 410  ######### Train Loss: 1.4297106564044952  ######### Relative L2 Test Norm: 4.099802672863007\n","######### Epoch: 411  ######### Train Loss: 1.4303004145622253  ######### Relative L2 Test Norm: 4.108342349529266\n","######### Epoch: 412  ######### Train Loss: 1.434046357870102  ######### Relative L2 Test Norm: 4.103686809539795\n","######### Epoch: 413  ######### Train Loss: 1.4270521700382233  ######### Relative L2 Test Norm: 4.124776482582092\n","######### Epoch: 414  ######### Train Loss: 1.4312695562839508  ######### Relative L2 Test Norm: 4.095293402671814\n","######### Epoch: 415  ######### Train Loss: 1.4362586736679077  ######### Relative L2 Test Norm: 4.092923700809479\n","######### Epoch: 416  ######### Train Loss: 1.4275259971618652  ######### Relative L2 Test Norm: 4.08981865644455\n","######### Epoch: 417  ######### Train Loss: 1.4496247470378876  ######### Relative L2 Test Norm: 4.083436965942383\n","######### Epoch: 418  ######### Train Loss: 1.4330803751945496  ######### Relative L2 Test Norm: 4.1145254373550415\n","######### Epoch: 419  ######### Train Loss: 1.428329885005951  ######### Relative L2 Test Norm: 4.1066465973854065\n","######### Epoch: 420  ######### Train Loss: 1.4267804026603699  ######### Relative L2 Test Norm: 4.108016669750214\n","######### Epoch: 421  ######### Train Loss: 1.434262901544571  ######### Relative L2 Test Norm: 4.116032898426056\n","######### Epoch: 422  ######### Train Loss: 1.4244281649589539  ######### Relative L2 Test Norm: 4.084602236747742\n","######### Epoch: 423  ######### Train Loss: 1.431519865989685  ######### Relative L2 Test Norm: 4.108461976051331\n","######### Epoch: 424  ######### Train Loss: 1.4332238733768463  ######### Relative L2 Test Norm: 4.1154637932777405\n","######### Epoch: 425  ######### Train Loss: 1.4364015460014343  ######### Relative L2 Test Norm: 4.106364011764526\n","######### Epoch: 426  ######### Train Loss: 1.4415123760700226  ######### Relative L2 Test Norm: 4.089445948600769\n","######### Epoch: 427  ######### Train Loss: 1.4232812821865082  ######### Relative L2 Test Norm: 4.045993149280548\n","######### Epoch: 428  ######### Train Loss: 1.4278648495674133  ######### Relative L2 Test Norm: 4.0718629360198975\n","######### Epoch: 429  ######### Train Loss: 1.426045298576355  ######### Relative L2 Test Norm: 4.078955769538879\n","######### Epoch: 430  ######### Train Loss: 1.4368768334388733  ######### Relative L2 Test Norm: 4.10905659198761\n","######### Epoch: 431  ######### Train Loss: 1.4314132928848267  ######### Relative L2 Test Norm: 4.131634294986725\n","######### Epoch: 432  ######### Train Loss: 1.4326146245002747  ######### Relative L2 Test Norm: 4.1259021162986755\n","######### Epoch: 433  ######### Train Loss: 1.4293414950370789  ######### Relative L2 Test Norm: 4.09950190782547\n","######### Epoch: 434  ######### Train Loss: 1.4317187070846558  ######### Relative L2 Test Norm: 4.131000816822052\n","######### Epoch: 435  ######### Train Loss: 1.4244226217269897  ######### Relative L2 Test Norm: 4.071353793144226\n","######### Epoch: 436  ######### Train Loss: 1.4241247177124023  ######### Relative L2 Test Norm: 4.093167781829834\n","######### Epoch: 437  ######### Train Loss: 1.4240556061267853  ######### Relative L2 Test Norm: 4.0839807987213135\n","######### Epoch: 438  ######### Train Loss: 1.4232729971408844  ######### Relative L2 Test Norm: 4.132525861263275\n","######### Epoch: 439  ######### Train Loss: 1.4278322160243988  ######### Relative L2 Test Norm: 4.068482875823975\n","######### Epoch: 440  ######### Train Loss: 1.4284848868846893  ######### Relative L2 Test Norm: 4.109946846961975\n","######### Epoch: 441  ######### Train Loss: 1.4211589992046356  ######### Relative L2 Test Norm: 4.105810105800629\n","######### Epoch: 442  ######### Train Loss: 1.432694435119629  ######### Relative L2 Test Norm: 4.10756903886795\n","######### Epoch: 443  ######### Train Loss: 1.4232247471809387  ######### Relative L2 Test Norm: 4.091105043888092\n","######### Epoch: 444  ######### Train Loss: 1.4219930469989777  ######### Relative L2 Test Norm: 4.092145204544067\n","######### Epoch: 445  ######### Train Loss: 1.4349197149276733  ######### Relative L2 Test Norm: 4.100291550159454\n","######### Epoch: 446  ######### Train Loss: 1.4278956055641174  ######### Relative L2 Test Norm: 4.098529696464539\n","######### Epoch: 447  ######### Train Loss: 1.4302565157413483  ######### Relative L2 Test Norm: 4.0891008377075195\n","######### Epoch: 448  ######### Train Loss: 1.4402439296245575  ######### Relative L2 Test Norm: 4.097651898860931\n","######### Epoch: 449  ######### Train Loss: 1.431306391954422  ######### Relative L2 Test Norm: 4.093204975128174\n","######### Epoch: 450  ######### Train Loss: 1.4217596650123596  ######### Relative L2 Test Norm: 4.100884556770325\n","######### Epoch: 451  ######### Train Loss: 1.4212417304515839  ######### Relative L2 Test Norm: 4.1000049114227295\n","######### Epoch: 452  ######### Train Loss: 1.4206877946853638  ######### Relative L2 Test Norm: 4.094227969646454\n","######### Epoch: 453  ######### Train Loss: 1.439392626285553  ######### Relative L2 Test Norm: 4.091918766498566\n","######### Epoch: 454  ######### Train Loss: 1.4211466014385223  ######### Relative L2 Test Norm: 4.085490345954895\n","######### Epoch: 455  ######### Train Loss: 1.4305739402770996  ######### Relative L2 Test Norm: 4.091974377632141\n","######### Epoch: 456  ######### Train Loss: 1.4203591048717499  ######### Relative L2 Test Norm: 4.083875417709351\n","######### Epoch: 457  ######### Train Loss: 1.417857140302658  ######### Relative L2 Test Norm: 4.107312738895416\n","######### Epoch: 458  ######### Train Loss: 1.421442985534668  ######### Relative L2 Test Norm: 4.082782506942749\n","######### Epoch: 459  ######### Train Loss: 1.4293202459812164  ######### Relative L2 Test Norm: 4.067162811756134\n","######### Epoch: 460  ######### Train Loss: 1.417503833770752  ######### Relative L2 Test Norm: 4.105677902698517\n","######### Epoch: 461  ######### Train Loss: 1.449925184249878  ######### Relative L2 Test Norm: 4.106343686580658\n","######### Epoch: 462  ######### Train Loss: 1.43180450797081  ######### Relative L2 Test Norm: 4.113951802253723\n","######### Epoch: 463  ######### Train Loss: 1.4269194900989532  ######### Relative L2 Test Norm: 4.108848512172699\n","######### Epoch: 464  ######### Train Loss: 1.4200572073459625  ######### Relative L2 Test Norm: 4.087320327758789\n","######### Epoch: 465  ######### Train Loss: 1.440934956073761  ######### Relative L2 Test Norm: 4.097982585430145\n","######### Epoch: 466  ######### Train Loss: 1.4242950975894928  ######### Relative L2 Test Norm: 4.094353199005127\n","######### Epoch: 467  ######### Train Loss: 1.4314134120941162  ######### Relative L2 Test Norm: 4.106438994407654\n","######### Epoch: 468  ######### Train Loss: 1.4194805026054382  ######### Relative L2 Test Norm: 4.1071794629096985\n","######### Epoch: 469  ######### Train Loss: 1.4233916699886322  ######### Relative L2 Test Norm: 4.1051384806633\n","######### Epoch: 470  ######### Train Loss: 1.4221549332141876  ######### Relative L2 Test Norm: 4.103194952011108\n","######### Epoch: 471  ######### Train Loss: 1.4289245903491974  ######### Relative L2 Test Norm: 4.097802102565765\n","######### Epoch: 472  ######### Train Loss: 1.4185550510883331  ######### Relative L2 Test Norm: 4.102458357810974\n","######### Epoch: 473  ######### Train Loss: 1.4219217598438263  ######### Relative L2 Test Norm: 4.125372052192688\n","######### Epoch: 474  ######### Train Loss: 1.42879256606102  ######### Relative L2 Test Norm: 4.112093091011047\n","######### Epoch: 475  ######### Train Loss: 1.4420363008975983  ######### Relative L2 Test Norm: 4.1041523814201355\n","######### Epoch: 476  ######### Train Loss: 1.4227089881896973  ######### Relative L2 Test Norm: 4.051413893699646\n","######### Epoch: 477  ######### Train Loss: 1.4198578298091888  ######### Relative L2 Test Norm: 4.090510606765747\n","######### Epoch: 478  ######### Train Loss: 1.4247299134731293  ######### Relative L2 Test Norm: 4.0758578181266785\n","######### Epoch: 479  ######### Train Loss: 1.4227065443992615  ######### Relative L2 Test Norm: 4.0579299330711365\n","######### Epoch: 480  ######### Train Loss: 1.4171571135520935  ######### Relative L2 Test Norm: 4.115614652633667\n","######### Epoch: 481  ######### Train Loss: 1.4224381744861603  ######### Relative L2 Test Norm: 4.099710941314697\n","######### Epoch: 482  ######### Train Loss: 1.4168435335159302  ######### Relative L2 Test Norm: 4.096650779247284\n","######### Epoch: 483  ######### Train Loss: 1.4251552522182465  ######### Relative L2 Test Norm: 4.089697301387787\n","######### Epoch: 484  ######### Train Loss: 1.424649566411972  ######### Relative L2 Test Norm: 4.079429864883423\n","######### Epoch: 485  ######### Train Loss: 1.4218810498714447  ######### Relative L2 Test Norm: 4.093691289424896\n","######### Epoch: 486  ######### Train Loss: 1.421112984418869  ######### Relative L2 Test Norm: 4.09070760011673\n","######### Epoch: 487  ######### Train Loss: 1.4200345277786255  ######### Relative L2 Test Norm: 4.097490310668945\n","######### Epoch: 488  ######### Train Loss: 1.4213179349899292  ######### Relative L2 Test Norm: 4.099161326885223\n","######### Epoch: 489  ######### Train Loss: 1.4193168580532074  ######### Relative L2 Test Norm: 4.125985324382782\n","######### Epoch: 490  ######### Train Loss: 1.4314848184585571  ######### Relative L2 Test Norm: 4.082359313964844\n","######### Epoch: 491  ######### Train Loss: 1.416241616010666  ######### Relative L2 Test Norm: 4.096026718616486\n","######### Epoch: 492  ######### Train Loss: 1.433119297027588  ######### Relative L2 Test Norm: 4.071548044681549\n","######### Epoch: 493  ######### Train Loss: 1.4130208790302277  ######### Relative L2 Test Norm: 4.077827274799347\n","######### Epoch: 494  ######### Train Loss: 1.4170653522014618  ######### Relative L2 Test Norm: 4.0754464864730835\n","######### Epoch: 495  ######### Train Loss: 1.421374350786209  ######### Relative L2 Test Norm: 4.110269129276276\n","######### Epoch: 496  ######### Train Loss: 1.4204756915569305  ######### Relative L2 Test Norm: 4.089956521987915\n","######### Epoch: 497  ######### Train Loss: 1.4222883582115173  ######### Relative L2 Test Norm: 4.08268016576767\n","######### Epoch: 498  ######### Train Loss: 1.4211595356464386  ######### Relative L2 Test Norm: 4.088513255119324\n","######### Epoch: 499  ######### Train Loss: 1.4203124642372131  ######### Relative L2 Test Norm: 4.088668763637543\n"]}],"source":["optimizer = AdamW(fno.parameters(), lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","\n","\n","#l = torch.nn.MSELoss()\n","freq_print = 1\n","for epoch in range(epochs):\n","    train_mse = 0.0\n","    for step, (input_batch, output_batch) in enumerate(training_set):\n","        optimizer.zero_grad()\n","        output_pred_batch = fno(input_batch).squeeze(2)\n","        loss_f = l2loss(output_pred_batch, output_batch)\n","        loss_f.backward()\n","        optimizer.step()\n","        train_mse += loss_f.item()\n","    train_mse /= len(training_set)\n","\n","    scheduler.step()\n","\n","    with torch.no_grad():\n","        fno.eval()\n","        test_relative_l2 = 0.0\n","        for step, (input_batch, output_batch) in enumerate(testing_set):\n","            output_pred_batch = fno(input_batch).squeeze(2)\n","            loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","            test_relative_l2 += loss_f.item()\n","        test_relative_l2 /= len(testing_set)\n","\n","    if epoch % freq_print == 0: print(\"######### Epoch:\", epoch, \" ######### Train Loss:\", train_mse, \" ######### Relative L2 Test Norm:\", test_relative_l2)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eoig--XVdgUc","colab":{"base_uri":"https://localhost:8080/","height":466},"executionInfo":{"status":"ok","timestamp":1735305515858,"user_tz":-60,"elapsed":541,"user":{"displayName":"ghets 79","userId":"11905067178965594936"}},"outputId":"dab64b30-744a-4c97-c26e-79f9d741ae40"},"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 error:  3.056262969970703\n"]},{"output_type":"execute_result","data":{"text/plain":["<matplotlib.legend.Legend at 0x7d1524696e30>"]},"metadata":{},"execution_count":19},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["idx_data = 34\n","input_function_test_n = input_test[idx_data, :].unsqueeze(0)\n","output_function_test_n = output_test[idx_data, :].unsqueeze(0)\n","\n","output_function_test_pred_n = fno(input_function_test_n)\n","\n","plt.figure()\n","plt.grid(True, which=\"both\", ls=\":\")\n","plt.plot(torch.linspace(0,1,64), output_function_test_n[0].detach(), label=\"True Solution\", c=\"C0\", lw=2)\n","plt.scatter(torch.linspace(0,1,64), output_function_test_pred_n[0].detach(), label=\"Approximate Solution\", s=8, c=\"C1\")\n","p = 2\n","err = (torch.mean(abs(output_function_test_n.detach().reshape(-1, ) - output_function_test_pred_n.detach().reshape(-1, )) ** p) / torch.mean(abs(output_function_test_n.detach()) ** p)) ** (1 / p) * 100\n","print(\"Relative L2 error: \", err.item())\n","plt.legend()"]},{"cell_type":"markdown","metadata":{"id":"YS-FQThaGtI0"},"source":["Testing on the sol dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":185,"status":"ok","timestamp":1735305536540,"user":{"displayName":"ghets 79","userId":"11905067178965594936"},"user_tz":-60},"id":"h0At4Uwt1IXT","outputId":"bde35447-3eb6-40a2-c26b-fb26501ddb03"},"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 Test Norm: 4.187814712524414\n"]}],"source":["x_sol=torch.from_numpy(np.load('test_sol.npy')).type(torch.float32)[:,0,:]\n","y_sol=torch.from_numpy(np.load('test_sol.npy')).type(torch.float32)[:,4,:]\n","\n","testing_dataset=DataLoader(TensorDataset(x_sol,y_sol),batch_size=len(x_sol))\n","\n","with torch.no_grad():\n","    fno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch) in enumerate(testing_dataset):\n","        output_pred_batch = fno(input_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","        test_relative_l2 /= len(testing_dataset)\n","    print(\"Relative L2 Test Norm:\", test_relative_l2)\n","    l_test=test_relative_l2"]},{"cell_type":"markdown","metadata":{"id":"GAcGprHdGmZt"},"source":["Resolution 32:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":186,"status":"ok","timestamp":1735305539343,"user":{"displayName":"ghets 79","userId":"11905067178965594936"},"user_tz":-60},"id":"J_5LVtUdCZum","outputId":"5323ba62-05bc-4038-96f8-5b4f2f0628e0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 Test Norm: 8.392383575439453\n"]}],"source":["x_32=torch.from_numpy(np.load('test_sol_res_32.npy')).type(torch.float32)[:,0,:]\n","y_32=torch.from_numpy(np.load('test_sol_res_32.npy')).type(torch.float32)[:,1,:]\n","\n","testing_32=DataLoader(TensorDataset(x_32,y_32),batch_size=len(x_32))\n","\n","with torch.no_grad():\n","    fno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch) in enumerate(testing_32):\n","        output_pred_batch = fno(input_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","        test_relative_l2 /= len(testing_32)\n","    print(\"Relative L2 Test Norm:\", test_relative_l2)\n","    l_32=test_relative_l2"]},{"cell_type":"markdown","metadata":{"id":"JrwWp6IDGjty"},"source":["Resolution 64:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":418,"status":"ok","timestamp":1735305543206,"user":{"displayName":"ghets 79","userId":"11905067178965594936"},"user_tz":-60},"id":"kHytCugaCZm4","outputId":"673c4d2b-6f73-45e7-e032-dc3161eba5eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 Test Norm: 4.418882369995117\n"]}],"source":["x_64=torch.from_numpy(np.load('test_sol_res_64.npy')).type(torch.float32)[:,0,:]\n","y_64=torch.from_numpy(np.load('test_sol_res_64.npy')).type(torch.float32)[:,1,:]\n","testing_64=DataLoader(TensorDataset(x_64,y_64),batch_size=len(x_64))\n","\n","with torch.no_grad():\n","    fno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch) in enumerate(testing_64):\n","        output_pred_batch = fno(input_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","        test_relative_l2 /= len(testing_64)\n","    print(\"Relative L2 Test Norm:\", test_relative_l2)\n","    l_64=test_relative_l2"]},{"cell_type":"markdown","metadata":{"id":"hCMGRlq2Ghnw"},"source":["Resolution 96:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":253,"status":"ok","timestamp":1735305545580,"user":{"displayName":"ghets 79","userId":"11905067178965594936"},"user_tz":-60},"id":"Vgkt7eNdDwAl","outputId":"6d876bd3-17b1-4314-bec6-048fb1f75489"},"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 Test Norm: 4.769369125366211\n"]}],"source":["x_96=torch.from_numpy(np.load('test_sol_res_96.npy')).type(torch.float32)[:,0,:]\n","y_96=torch.from_numpy(np.load('test_sol_res_96.npy')).type(torch.float32)[:,1,:]\n","testing_96=DataLoader(TensorDataset(x_96,y_96),batch_size=len(x_96))\n","\n","with torch.no_grad():\n","    fno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch) in enumerate(testing_96):\n","        output_pred_batch = fno(input_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","        test_relative_l2 /= len(testing_96)\n","    print(\"Relative L2 Test Norm:\", test_relative_l2)\n","    l_96=test_relative_l2\n"]},{"cell_type":"markdown","metadata":{"id":"NwxzmtkDGemp"},"source":["Resolution 128:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":167,"status":"ok","timestamp":1735305549254,"user":{"displayName":"ghets 79","userId":"11905067178965594936"},"user_tz":-60},"id":"HjCXw1dZCZhp","outputId":"ba6d92c9-3dc3-47bf-be13-1c53b0b4af86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 Test Norm: 5.300142288208008\n"]}],"source":["x_128=torch.from_numpy(np.load('test_sol_res_128.npy')).type(torch.float32)[:,0,:]\n","y_128=torch.from_numpy(np.load('test_sol_res_128.npy')).type(torch.float32)[:,1,:]\n","testing_128=DataLoader(TensorDataset(x_128,y_128),batch_size=len(x_128))\n","\n","with torch.no_grad():\n","    fno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch) in enumerate(testing_128):\n","        output_pred_batch = fno(input_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","        test_relative_l2 /= len(testing_128)\n","    print(\"Relative L2 Test Norm:\", test_relative_l2)\n","    l_128=test_relative_l2\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ckvvYkLuJe2J"},"source":["Overall the average L2 error seems to be coeherent around the trained resolution, at 64 and 96. However the error notably increases at 128, and at 32 it peaks at around 8/9%. Therefore the error is not mesh-independent."]},{"cell_type":"markdown","metadata":{"id":"XlG3GWFwGYie"},"source":["Out-of-distribution testing:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":189,"status":"ok","timestamp":1735305551608,"user":{"displayName":"ghets 79","userId":"11905067178965594936"},"user_tz":-60},"id":"aKv52WtHKcFn","outputId":"404529fd-9600-46a0-fb75-c23ee889ebee"},"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 Test Norm: 8.577457427978516\n"]}],"source":["x_ood=torch.from_numpy(np.load('test_sol_OOD.npy')).type(torch.float32)[:,0,:]\n","y_ood=torch.from_numpy(np.load('test_sol_OOD.npy')).type(torch.float32)[:,1,:]\n","testing_ood=DataLoader(TensorDataset(x_ood,y_ood),batch_size=len(x_ood))\n","\n","with torch.no_grad():\n","    fno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch) in enumerate(testing_ood):\n","        output_pred_batch = fno(input_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","        test_relative_l2 /= len(testing_ood)\n","    print(\"Relative L2 Test Norm:\", test_relative_l2)\n","    l_ood=test_relative_l2\n"]},{"cell_type":"markdown","metadata":{"id":"my59PMtDTR9P"},"source":["The out-of-distribution error is 3/4% higher than the validation error of task1"]},{"cell_type":"markdown","metadata":{"id":"qTZ7ZQnKGTWg"},"source":["Building the all2all dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TYjzZCYPGlfE"},"outputs":[],"source":["#---------------------\n","# Time-conditional BN:\n","#---------------------\n","\n","class FILM(torch.nn.Module):\n","    def __init__(self,\n","                channels,\n","                use_bn = True):\n","        super(FILM, self).__init__()\n","        self.channels = channels\n","        self.inp2scale = nn.Linear(in_features=1, out_features=channels, bias=True)\n","        self.inp2bias = nn.Linear(in_features=1, out_features=channels, bias=True)\n","\n","        if use_bn:\n","            self.norm = nn.BatchNorm1d(channels)\n","        else:\n","            self.norm = nn.Identity()\n","\n","    def forward(self, x, time):\n","\n","        x = self.norm(x)\n","        time = time.reshape(-1,1).type_as(x)\n","        scale     = self.inp2scale(time)\n","        bias      = self.inp2bias(time)\n","        scale = scale.unsqueeze(2).expand_as(x)\n","        bias  = bias.unsqueeze(2).expand_as(x)\n","\n","        return x * scale + bias"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LHZhQmNZkRUx"},"outputs":[],"source":["class FNO1d_2(nn.Module):\n","    def __init__(self, modes, width):\n","        super(FNO1d_2, self).__init__()\n","\n","        self.modes1 = modes\n","        self.width = width\n","        self.padding = 1  # pad the domain if input is non-periodic\n","        self.norm0=FILM(self.width)\n","        self.norm2=FILM(self.width)\n","        self.norm4=FILM(self.width)\n","        self.norm6=FILM(self.width)\n","        self.norm8=FILM(self.width)\n","        self.norm10=FILM(self.width)\n","\n","        self.linear_p = nn.Linear(2, self.width)  # input channel is 2: (u0(x), x) --> GRID IS INCLUDED!\n","\n","        self.spect0 = SpectralConv1d(self.width, self.width, self.modes1)\n","        self.spect2 = SpectralConv1d(self.width, self.width, self.modes1)\n","        self.spect4 = SpectralConv1d(self.width, self.width, self.modes1)\n","        self.spect6 = SpectralConv1d(self.width, self.width, self.modes1)\n","        self.spect8 = SpectralConv1d(self.width, self.width, self.modes1)\n","        self.spect10 = SpectralConv1d(self.width, self.width, self.modes1)\n","\n","        self.lin0 = nn.Conv1d(self.width, self.width, 1)\n","        self.lin2 = nn.Conv1d(self.width, self.width, 1)\n","        self.lin4 = nn.Conv1d(self.width, self.width, 1)\n","        self.lin6 = nn.Conv1d(self.width, self.width, 1)\n","        self.lin8 = nn.Conv1d(self.width, self.width, 1)\n","        self.lin10 = nn.Conv1d(self.width, self.width, 1)\n","\n","        self.linear_q = nn.Linear(self.width, 32)\n","        self.output_layer = nn.Linear(32, 1)\n","\n","        self.activation = torch.nn.GELU()\n","\n","    def get_grid(self,x):\n","        res=x.shape[1]\n","        grid=torch.linspace(0,1,res)\n","        temporary_tensor=torch.zeros(x.shape[0], res, 2)\n","        temporary_tensor[:,:,0]=x.squeeze()\n","        for i in range(x.shape[0]):\n","            temporary_tensor[i,:,1]=grid\n","        return temporary_tensor\n","\n","    def fourier_layer(self, x, spectral_layer1, conv_layer1,time,bn1):\n","        out=spectral_layer1(x)+conv_layer1(x) #global and local convolution\n","        out=bn1(out,time) #normalize with time\n","        out=self.activation(out)\n","        return out\n","\n","    def linear_layer(self, x, linear_transformation):\n","        return self.activation(linear_transformation(x))\n","\n","    def forward(self, x, time):\n","        x = self.get_grid(x)\n","        x = self.linear_p(x)\n","        x = x.permute(0, 2, 1)\n","\n","        x = F.pad(x, [0, self.padding])\n","        x = self.fourier_layer(x, self.spect0, self.lin0, time, self.norm0)\n","        x = self.fourier_layer(x, self.spect2, self.lin2, time, self.norm2)\n","        x = self.fourier_layer(x, self.spect4, self.lin4, time, self.norm4)\n","        x = self.fourier_layer(x, self.spect6, self.lin6, time, self.norm6)\n","        x = self.fourier_layer(x, self.spect10, self.lin10, time, self.norm10)\n","        x = x[..., :-self.padding]\n","        x = x.permute(0, 2, 1)\n","\n","        x = self.linear_layer(x, self.linear_q)\n","        x = self.output_layer(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qYIukzryTQy8"},"outputs":[],"source":["n=64\n","all_snap_train=torch.from_numpy(np.load('train_sol.npy')).type(torch.float32)[:n,:,:] #training with all time snapshots 64x5x64\n","time_indexes=torch.tensor([(i, j) for i in range(5) for j in range(i+1,5) if i<4]) #all valid indexes pairs\n","all2all_input_train=torch.zeros(len(time_indexes)*n,64)\n","all2all_output_train=torch.zeros(len(time_indexes)*n,64)\n","time_step_train=torch.zeros(len(time_indexes)*64)\n","q=0\n","for k in range(n):\n","    for i,j in time_indexes:\n","        all2all_input_train[q,:]=all_snap_train[k,i,:] #input at time i\n","        all2all_output_train[q,:]=all_snap_train[k,j,:] #output at time j\n","        time_step_train[q]=(j-i)/4   #delta t\n","        q+=1\n","\n","batch_s=32\n","all2all_train_dat=DataLoader(TensorDataset(all2all_input_train,all2all_output_train,time_step_train),batch_size=batch_s,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_ZYVoxCTtYa"},"outputs":[],"source":["all_snap_test=torch.from_numpy(np.load('train_sol.npy')).type(torch.float32)[n:,:,:]  #validation 64x5x64\n","all2all_input_test=torch.zeros(64,64)\n","all2all_output_test=torch.zeros(64,64)\n","time_step_test=torch.zeros(64)\n","q=0\n","for k in range(n):\n","    all2all_input_test[q,:]=all_snap_test[k,0,:]\n","    all2all_output_test[q,:]=all_snap_test[k,4,:] #only prediction at t=1\n","    time_step_test[q]=1\n","    q+=1\n","\n","all2all_test_dat=DataLoader(TensorDataset(all2all_input_test, all2all_output_test, time_step_test),batch_size=batch_s,shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RjV73KHOPHci"},"outputs":[],"source":["learning_rate = 0.001\n","epochs= 100\n","step_size = 50\n","gamma = 0.50\n","allfno=FNO1d_2(16,64)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hNVU0epHm_I0","outputId":"5c3362dd-6b77-47ba-db36-c2e96fdd8cc0","executionInfo":{"status":"ok","timestamp":1735308155971,"user_tz":-60,"elapsed":158248,"user":{"displayName":"ghets 79","userId":"11905067178965594936"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["######### Epoch: 0  ######### Train Loss: 59.74507160186768  ######### Relative L2 Test Norm: 88.8097915649414\n","######### Epoch: 1  ######### Train Loss: 70.18993129730225  ######### Relative L2 Test Norm: 79.77299499511719\n","######### Epoch: 2  ######### Train Loss: 47.61732997894287  ######### Relative L2 Test Norm: 48.1203670501709\n","######### Epoch: 3  ######### Train Loss: 43.77990379333496  ######### Relative L2 Test Norm: 51.48957633972168\n","######### Epoch: 4  ######### Train Loss: 42.833760833740236  ######### Relative L2 Test Norm: 40.602542877197266\n","######### Epoch: 5  ######### Train Loss: 38.85061798095703  ######### Relative L2 Test Norm: 37.666534423828125\n","######### Epoch: 6  ######### Train Loss: 37.99378299713135  ######### Relative L2 Test Norm: 39.6622428894043\n","######### Epoch: 7  ######### Train Loss: 37.59634199142456  ######### Relative L2 Test Norm: 35.1879825592041\n","######### Epoch: 8  ######### Train Loss: 37.17519559860229  ######### Relative L2 Test Norm: 36.25202560424805\n","######### Epoch: 9  ######### Train Loss: 36.079868984222415  ######### Relative L2 Test Norm: 32.51925277709961\n","######### Epoch: 10  ######### Train Loss: 36.03279886245728  ######### Relative L2 Test Norm: 30.079280853271484\n","######### Epoch: 11  ######### Train Loss: 35.42748622894287  ######### Relative L2 Test Norm: 32.43088150024414\n","######### Epoch: 12  ######### Train Loss: 33.65550680160523  ######### Relative L2 Test Norm: 31.569232940673828\n","######### Epoch: 13  ######### Train Loss: 32.896480560302734  ######### Relative L2 Test Norm: 31.097779273986816\n","######### Epoch: 14  ######### Train Loss: 32.27453775405884  ######### Relative L2 Test Norm: 40.50426483154297\n","######### Epoch: 15  ######### Train Loss: 32.281046867370605  ######### Relative L2 Test Norm: 31.405062675476074\n","######### Epoch: 16  ######### Train Loss: 31.442296314239503  ######### Relative L2 Test Norm: 32.8471565246582\n","######### Epoch: 17  ######### Train Loss: 30.40240135192871  ######### Relative L2 Test Norm: 34.971872329711914\n","######### Epoch: 18  ######### Train Loss: 29.987525177001952  ######### Relative L2 Test Norm: 35.11443901062012\n","######### Epoch: 19  ######### Train Loss: 29.088882637023925  ######### Relative L2 Test Norm: 29.580891609191895\n","######### Epoch: 20  ######### Train Loss: 28.55102882385254  ######### Relative L2 Test Norm: 32.00130653381348\n","######### Epoch: 21  ######### Train Loss: 26.3916955947876  ######### Relative L2 Test Norm: 32.677873611450195\n","######### Epoch: 22  ######### Train Loss: 24.533219051361083  ######### Relative L2 Test Norm: 32.28294563293457\n","######### Epoch: 23  ######### Train Loss: 23.820883178710936  ######### Relative L2 Test Norm: 32.83123779296875\n","######### Epoch: 24  ######### Train Loss: 23.434647369384766  ######### Relative L2 Test Norm: 31.44631290435791\n","######### Epoch: 25  ######### Train Loss: 21.961393451690675  ######### Relative L2 Test Norm: 31.17057991027832\n","######### Epoch: 26  ######### Train Loss: 20.311003303527833  ######### Relative L2 Test Norm: 37.53909873962402\n","######### Epoch: 27  ######### Train Loss: 18.999329471588133  ######### Relative L2 Test Norm: 31.700016021728516\n","######### Epoch: 28  ######### Train Loss: 16.92728695869446  ######### Relative L2 Test Norm: 34.74510192871094\n","######### Epoch: 29  ######### Train Loss: 15.817922067642211  ######### Relative L2 Test Norm: 33.19347858428955\n","######### Epoch: 30  ######### Train Loss: 14.855706596374512  ######### Relative L2 Test Norm: 30.317082405090332\n","######### Epoch: 31  ######### Train Loss: 14.259604930877686  ######### Relative L2 Test Norm: 31.699227333068848\n","######### Epoch: 32  ######### Train Loss: 13.020394897460937  ######### Relative L2 Test Norm: 30.297021865844727\n","######### Epoch: 33  ######### Train Loss: 12.8941246509552  ######### Relative L2 Test Norm: 32.45004844665527\n","######### Epoch: 34  ######### Train Loss: 13.897095203399658  ######### Relative L2 Test Norm: 33.22408676147461\n","######### Epoch: 35  ######### Train Loss: 12.394071769714355  ######### Relative L2 Test Norm: 31.896154403686523\n","######### Epoch: 36  ######### Train Loss: 11.559300661087036  ######### Relative L2 Test Norm: 32.86703872680664\n","######### Epoch: 37  ######### Train Loss: 11.655716562271119  ######### Relative L2 Test Norm: 31.333602905273438\n","######### Epoch: 38  ######### Train Loss: 10.802700996398926  ######### Relative L2 Test Norm: 32.59793949127197\n","######### Epoch: 39  ######### Train Loss: 9.866327905654908  ######### Relative L2 Test Norm: 33.51447296142578\n","######### Epoch: 40  ######### Train Loss: 9.453067898750305  ######### Relative L2 Test Norm: 31.825674057006836\n","######### Epoch: 41  ######### Train Loss: 9.404350471496581  ######### Relative L2 Test Norm: 31.699118614196777\n","######### Epoch: 42  ######### Train Loss: 9.93567852973938  ######### Relative L2 Test Norm: 30.37654972076416\n","######### Epoch: 43  ######### Train Loss: 9.410480308532716  ######### Relative L2 Test Norm: 30.1479549407959\n","######### Epoch: 44  ######### Train Loss: 9.140018773078918  ######### Relative L2 Test Norm: 32.24811363220215\n","######### Epoch: 45  ######### Train Loss: 8.787608075141907  ######### Relative L2 Test Norm: 31.414156913757324\n","######### Epoch: 46  ######### Train Loss: 7.970100903511048  ######### Relative L2 Test Norm: 29.609272003173828\n","######### Epoch: 47  ######### Train Loss: 7.982559633255005  ######### Relative L2 Test Norm: 30.17106342315674\n","######### Epoch: 48  ######### Train Loss: 8.198859930038452  ######### Relative L2 Test Norm: 31.765645027160645\n","######### Epoch: 49  ######### Train Loss: 8.06930320262909  ######### Relative L2 Test Norm: 28.903340339660645\n","######### Epoch: 50  ######### Train Loss: 6.5542655229568485  ######### Relative L2 Test Norm: 30.41346836090088\n","######### Epoch: 51  ######### Train Loss: 5.231526803970337  ######### Relative L2 Test Norm: 30.00477409362793\n","######### Epoch: 52  ######### Train Loss: 4.703549337387085  ######### Relative L2 Test Norm: 30.081300735473633\n","######### Epoch: 53  ######### Train Loss: 4.877413964271545  ######### Relative L2 Test Norm: 30.17553424835205\n","######### Epoch: 54  ######### Train Loss: 4.431938743591308  ######### Relative L2 Test Norm: 30.59807300567627\n","######### Epoch: 55  ######### Train Loss: 4.131773054599762  ######### Relative L2 Test Norm: 30.49655532836914\n","######### Epoch: 56  ######### Train Loss: 4.10515832901001  ######### Relative L2 Test Norm: 30.309870719909668\n","######### Epoch: 57  ######### Train Loss: 3.846610414981842  ######### Relative L2 Test Norm: 29.542227745056152\n","######### Epoch: 58  ######### Train Loss: 3.8488785147666933  ######### Relative L2 Test Norm: 31.41015911102295\n","######### Epoch: 59  ######### Train Loss: 3.882488322257996  ######### Relative L2 Test Norm: 30.172736167907715\n","######### Epoch: 60  ######### Train Loss: 3.860072946548462  ######### Relative L2 Test Norm: 29.733631134033203\n","######### Epoch: 61  ######### Train Loss: 4.613953852653504  ######### Relative L2 Test Norm: 30.75278377532959\n","######### Epoch: 62  ######### Train Loss: 5.300414347648621  ######### Relative L2 Test Norm: 31.12347984313965\n","######### Epoch: 63  ######### Train Loss: 4.726745808124543  ######### Relative L2 Test Norm: 29.526801109313965\n","######### Epoch: 64  ######### Train Loss: 4.4431793808937075  ######### Relative L2 Test Norm: 30.407004356384277\n","######### Epoch: 65  ######### Train Loss: 4.1460702300071715  ######### Relative L2 Test Norm: 30.47697639465332\n","######### Epoch: 66  ######### Train Loss: 4.113489019870758  ######### Relative L2 Test Norm: 30.600821495056152\n","######### Epoch: 67  ######### Train Loss: 3.9273112416267395  ######### Relative L2 Test Norm: 29.908316612243652\n","######### Epoch: 68  ######### Train Loss: 3.8753784894943237  ######### Relative L2 Test Norm: 31.767218589782715\n","######### Epoch: 69  ######### Train Loss: 3.989426374435425  ######### Relative L2 Test Norm: 30.222888946533203\n","######### Epoch: 70  ######### Train Loss: 3.89581698179245  ######### Relative L2 Test Norm: 29.73430824279785\n","######### Epoch: 71  ######### Train Loss: 3.777278256416321  ######### Relative L2 Test Norm: 30.721022605895996\n","######### Epoch: 72  ######### Train Loss: 3.84006609916687  ######### Relative L2 Test Norm: 30.25938606262207\n","######### Epoch: 73  ######### Train Loss: 3.594702732563019  ######### Relative L2 Test Norm: 30.218040466308594\n","######### Epoch: 74  ######### Train Loss: 3.450323557853699  ######### Relative L2 Test Norm: 30.22244644165039\n","######### Epoch: 75  ######### Train Loss: 3.3847780346870424  ######### Relative L2 Test Norm: 30.745174407958984\n","######### Epoch: 76  ######### Train Loss: 4.054889416694641  ######### Relative L2 Test Norm: 29.945068359375\n","######### Epoch: 77  ######### Train Loss: 4.220657229423523  ######### Relative L2 Test Norm: 30.239815711975098\n","######### Epoch: 78  ######### Train Loss: 3.7959558367729187  ######### Relative L2 Test Norm: 30.124814987182617\n","######### Epoch: 79  ######### Train Loss: 3.340106427669525  ######### Relative L2 Test Norm: 30.239853858947754\n","######### Epoch: 80  ######### Train Loss: 3.6581005096435546  ######### Relative L2 Test Norm: 30.19127368927002\n","######### Epoch: 81  ######### Train Loss: 3.6862759828567504  ######### Relative L2 Test Norm: 30.40359401702881\n","######### Epoch: 82  ######### Train Loss: 3.5467178344726564  ######### Relative L2 Test Norm: 30.463808059692383\n","######### Epoch: 83  ######### Train Loss: 3.283169651031494  ######### Relative L2 Test Norm: 29.516265869140625\n","######### Epoch: 84  ######### Train Loss: 3.6758265495300293  ######### Relative L2 Test Norm: 29.899311065673828\n","######### Epoch: 85  ######### Train Loss: 3.4299893260002134  ######### Relative L2 Test Norm: 29.441024780273438\n","######### Epoch: 86  ######### Train Loss: 3.236226487159729  ######### Relative L2 Test Norm: 29.6682071685791\n","######### Epoch: 87  ######### Train Loss: 3.4280842900276185  ######### Relative L2 Test Norm: 30.138489723205566\n","######### Epoch: 88  ######### Train Loss: 3.845887005329132  ######### Relative L2 Test Norm: 29.7025785446167\n","######### Epoch: 89  ######### Train Loss: 3.828049051761627  ######### Relative L2 Test Norm: 30.77227783203125\n","######### Epoch: 90  ######### Train Loss: 3.7298582673072813  ######### Relative L2 Test Norm: 30.30148410797119\n","######### Epoch: 91  ######### Train Loss: 3.6640626192092896  ######### Relative L2 Test Norm: 29.68755340576172\n","######### Epoch: 92  ######### Train Loss: 3.64061781167984  ######### Relative L2 Test Norm: 30.610788345336914\n","######### Epoch: 93  ######### Train Loss: 3.4688933730125426  ######### Relative L2 Test Norm: 30.17687702178955\n","######### Epoch: 94  ######### Train Loss: 3.255346083641052  ######### Relative L2 Test Norm: 29.71945858001709\n","######### Epoch: 95  ######### Train Loss: 3.5896481037139893  ######### Relative L2 Test Norm: 29.95829963684082\n","######### Epoch: 96  ######### Train Loss: 4.016948473453522  ######### Relative L2 Test Norm: 30.060863494873047\n","######### Epoch: 97  ######### Train Loss: 3.843732166290283  ######### Relative L2 Test Norm: 30.75704574584961\n","######### Epoch: 98  ######### Train Loss: 3.6699208974838258  ######### Relative L2 Test Norm: 30.14688205718994\n","######### Epoch: 99  ######### Train Loss: 3.6707335233688356  ######### Relative L2 Test Norm: 29.77008819580078\n"]}],"source":["optimizer = AdamW(allfno.parameters(), lr=learning_rate)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n","\n","l = torch.nn.MSELoss()\n","freq_print = 1\n","for epoch in range(epochs):\n","    train_mse = 0.0\n","    for step, (input_batch, output_batch, time_batch) in enumerate(all2all_train_dat):\n","        optimizer.zero_grad()\n","        output_pred_batch = allfno(input_batch,time_batch).squeeze(2)\n","        loss_f = l2loss(output_pred_batch, output_batch)\n","        loss_f.backward()\n","        optimizer.step()\n","        train_mse += loss_f.item()\n","    train_mse /= len(all2all_train_dat)\n","\n","    scheduler.step()\n","\n","    with torch.no_grad():\n","        allfno.eval()\n","        test_relative_l2 = 0.0\n","        for step, (input_batch, output, time_batch) in enumerate(all2all_test_dat):\n","            output_pred_batch = allfno(input_batch,time_batch).squeeze(2)\n","            loss_f = (torch.mean((output_pred_batch - output) ** 2) / torch.mean(output ** 2)) ** 0.5 * 100\n","            test_relative_l2 += loss_f.item()\n","        test_relative_l2 /= len(all2all_test_dat)\n","\n","        if epoch % freq_print == 0: print(\"######### Epoch:\", epoch, \" ######### Train Loss:\", train_mse, \" ######### Relative L2 Test Norm:\", test_relative_l2)\n","\n"]},{"cell_type":"code","source":["all_snap_test=torch.from_numpy(np.load('test_sol.npy')).type(torch.float32)  #validation 64x5x64\n","all2all_25_input=torch.zeros(128,64)\n","all2all_25_output=torch.zeros(128,64)\n","time_25_test=torch.zeros(128)\n","q=0\n","for k in range(128):\n","    all2all_25_input[q,:]=all_snap_test[k,0,:]\n","    all2all_25_output[q,:]=all_snap_test[k,1,:]\n","    time_25_test[q]=0.25\n","    q+=1\n","\n","all2all_25_dat=DataLoader(TensorDataset(all2all_25_input, all2all_25_output, time_25_test),batch_size=len(all2all_25_input),shuffle=True)\n","\n","with torch.no_grad():\n","     allfno.eval()\n","     test_relative_l2 = 0.0\n","     for step, (input_batch, output_batch, time_batch) in enumerate(all2all_25_dat):\n","         output_pred_batch = allfno(input_batch,time_batch).squeeze(2)\n","         loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","         test_relative_l2 += loss_f.item()\n","     test_relative_l2 /= len(all2all_25_dat)\n","\n","     print(\" Relative L2 Test Norm:\", test_relative_l2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yYMWR5r4aCcR","executionInfo":{"status":"ok","timestamp":1735308492268,"user_tz":-60,"elapsed":172,"user":{"displayName":"ghets 79","userId":"11905067178965594936"}},"outputId":"35bf2887-f8ae-4da6-b56f-0647e9b343b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Relative L2 Test Norm: 20.965682983398438\n"]}]},{"cell_type":"code","source":["all_snap_test=torch.from_numpy(np.load('test_sol.npy')).type(torch.float32)  #validation 64x5x64\n","all2all_50_input=torch.zeros(128,64)\n","all2all_50_output=torch.zeros(128,64)\n","time_50_test=torch.zeros(128)\n","q=0\n","for k in range(128):\n","    all2all_50_input[q,:]=all_snap_test[k,0,:]\n","    all2all_50_output[q,:]=all_snap_test[k,2,:]\n","    time_50_test[q]=0.5\n","    q+=1\n","\n","all2all_50_dat=DataLoader(TensorDataset(all2all_50_input, all2all_50_output, time_50_test),batch_size=len(all2all_50_input),shuffle=True)\n","\n","with torch.no_grad():\n","    allfno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch, time_batch) in enumerate(all2all_50_dat):\n","        output_pred_batch = allfno(input_batch,time_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","    test_relative_l2 /= len(all2all_50_dat)\n","\n","    print(\" Relative L2 Test Norm:\", test_relative_l2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E-rbEHoZbaGJ","executionInfo":{"status":"ok","timestamp":1735308512007,"user_tz":-60,"elapsed":189,"user":{"displayName":"ghets 79","userId":"11905067178965594936"}},"outputId":"eec16167-60e5-4796-c702-aba8dec7c3b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Relative L2 Test Norm: 31.056018829345703\n"]}]},{"cell_type":"code","source":["all_snap_test=torch.from_numpy(np.load('test_sol.npy')).type(torch.float32)  #validation 64x5x64\n","all2all_75_input=torch.zeros(128,64)\n","all2all_75_output=torch.zeros(128,64)\n","time_75_test=torch.zeros(128)\n","q=0\n","for k in range(128):\n","    all2all_75_input[q,:]=all_snap_test[k,0,:]\n","    all2all_75_output[q,:]=all_snap_test[k,3,:]\n","    time_75_test[q]=0.75\n","    q+=1\n","\n","all2all_75_dat=DataLoader(TensorDataset(all2all_75_input, all2all_75_output, time_75_test),batch_size=len(all2all_75_input),shuffle=True)\n","\n","with torch.no_grad():\n","    allfno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch, time_batch) in enumerate(all2all_75_dat):\n","        output_pred_batch = allfno(input_batch,time_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","    test_relative_l2 /= len(all2all_75_dat)\n","\n","    print(\" Relative L2 Test Norm:\", test_relative_l2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FwldOq80b4BE","executionInfo":{"status":"ok","timestamp":1735308523751,"user_tz":-60,"elapsed":184,"user":{"displayName":"ghets 79","userId":"11905067178965594936"}},"outputId":"29427eab-16a4-4a3a-a35f-4961d7233015"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Relative L2 Test Norm: 38.615692138671875\n"]}]},{"cell_type":"code","source":["all_snap_test=torch.from_numpy(np.load('test_sol.npy')).type(torch.float32) #validation 64x5x64\n","all2all_input_test=torch.zeros(128,64)\n","all2all_output_test=torch.zeros(128,64)\n","time_step_test=torch.zeros(128)\n","q=0\n","for k in range(128):\n","    all2all_input_test[q,:]=all_snap_test[k,0,:]\n","    all2all_output_test[q,:]=all_snap_test[k,4,:]\n","    time_step_test[q]=1\n","    q+=1\n","\n","all2all_test_dat2=DataLoader(TensorDataset(all2all_input_test, all2all_output_test, time_step_test),batch_size=len(all2all_input_test),shuffle=True)\n","\n","with torch.no_grad():\n","    allfno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch, time_batch) in enumerate(all2all_test_dat2):\n","        output_pred_batch = allfno(input_batch,time_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","    test_relative_l2 /= len(all2all_test_dat2)\n","    print(\"Relative L2 Test Norm:\", test_relative_l2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J0xM1Q5leBqG","executionInfo":{"status":"ok","timestamp":1735308538469,"user_tz":-60,"elapsed":167,"user":{"displayName":"ghets 79","userId":"11905067178965594936"}},"outputId":"ca770c5c-0ad9-4537-cfc1-d1dfea6c34dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 Test Norm: 29.309627532958984\n"]}]},{"cell_type":"code","source":["x_ood=torch.from_numpy(np.load('test_sol_OOD.npy')).type(torch.float32)[:,0,:]\n","y_ood=torch.from_numpy(np.load('test_sol_OOD.npy')).type(torch.float32)[:,1,:]\n","time_step_ood=torch.ones(128)\n","testing_ood=DataLoader(TensorDataset(x_ood,y_ood,time_step_ood),batch_size=len(x_ood))\n","\n","with torch.no_grad():\n","    allfno.eval()\n","    test_relative_l2 = 0.0\n","    for step, (input_batch, output_batch, time_batch) in enumerate(testing_ood):\n","        output_pred_batch = allfno(input_batch,time_batch).squeeze(2)\n","        loss_f = (torch.mean((output_pred_batch - output_batch) ** 2) / torch.mean(output_batch ** 2)) ** 0.5 * 100\n","        test_relative_l2 += loss_f.item()\n","        test_relative_l2 /= len(testing_ood)\n","    print(\"Relative L2 Test Norm:\", test_relative_l2)\n","    l_ood=test_relative_l2\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4j59sDNbz_g","executionInfo":{"status":"ok","timestamp":1735308422861,"user_tz":-60,"elapsed":164,"user":{"displayName":"ghets 79","userId":"11905067178965594936"}},"outputId":"83ba520b-6747-4121-a751-43229665f346"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Relative L2 Test Norm: 35.69772720336914\n"]}]}],"metadata":{"colab":{"provenance":[{"file_id":"1UakFO2quXZBgUD0rzFavYL5x6bDxPbIZ","timestamp":1734346747553}],"authorship_tag":"ABX9TyNWnz681xPkqGgrPI9hIhon"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}